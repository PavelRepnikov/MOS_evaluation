{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f28df6c-fac3-47e8-8457-ee62321f061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import Wav2Vec2Model, HubertModel, WavLMModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import wandb\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from whisper import load_model\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb808b6-c3f8-4f50-9319-842844d3cffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fb6341-8550-49ee-9cbe-7f2c0b53899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Unfreeze Whisper‑medium ---\n",
    "whisper_model = whisper.load_model(\"base.en\").to(device)\n",
    "for param in whisper_model.parameters():\n",
    "    param.requires_grad = True\n",
    "whisper_model.train()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a74f2f2-029d-436d-bd96-c8c4795a0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: rtfiof (rtfiof-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>F:\\Project\\att_2\\wandb\\run-20250330_130939-koi6ohzp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl/runs/koi6ohzp' target=\"_blank\">finetune-whisper_b+bert+norm+enhanced</a></strong> to <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl' target=\"_blank\">https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl/runs/koi6ohzp' target=\"_blank\">https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl/runs/koi6ohzp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B online. Running your script from this directory will now sync to the cloud.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize wandb ---\n",
    "wandb.init(project=\"somos-ensemble2-ssl\", name=\"finetune-whisper_b+bert+norm+enhanced\")\n",
    "!wandb online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "302e5bf6-a061-469e-bd72-c1cdae6dafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "def load_json(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_mos_values(csv_path):\n",
    "    \"\"\"Load MOS values from the new normalized dataset.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    mos_dict = dict(zip(df.iloc[:, 0], df[\"new_scale\"]))  # Mapping: {audio_id: new_mos}\n",
    "    return mos_dict\n",
    "\n",
    "def load_transcripts(transcript_path):\n",
    "    \"\"\"Load transcripts into a dictionary for quick lookup.\"\"\"\n",
    "    transcript_dict = {}\n",
    "    with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                audio_id, text = parts\n",
    "                transcript_dict[audio_id] = text\n",
    "    return transcript_dict\n",
    "\n",
    "def process_audio_path(audio_id, base_dir=\"archive/update_SOMOS_v2/update_SOMOS_v2/all_audios/all_wavs\"):\n",
    "    \"\"\"Construct the full path to the audio file.\"\"\"\n",
    "    return os.path.join(base_dir, f\"{audio_id}.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f01dabc-9bf4-4325-92c2-66e2326d6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Class ---\n",
    "class SOMOSDataset(Dataset):\n",
    "    def __init__(self, csv_file, transcript_file, base_dir=\"archive/update_SOMOS_v2/update_SOMOS_v2/all_audios/all_wavs\", split=\"train\", test_size=0.2, seed=42):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transcripts = self.load_transcripts(transcript_file)\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "        # Use the new MOS scale\n",
    "        self.df[\"mos\"] = self.df[\"new_scale\"]\n",
    "\n",
    "        # Split data into train and validation sets\n",
    "        train_df, val_df = train_test_split(self.df, test_size=test_size, random_state=seed)\n",
    "        self.df = train_df if split == \"train\" else val_df\n",
    "\n",
    "    def load_transcripts(self, transcript_file):\n",
    "        transcripts = {}\n",
    "        with open(transcript_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                if len(parts) == 2:\n",
    "                    transcripts[parts[0]] = parts[1]\n",
    "        return transcripts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_name = row.iloc[0]  # First column is the file identifier\n",
    "        mos = torch.tensor(row[\"mos\"], dtype=torch.float)\n",
    "\n",
    "        # Get text from transcript\n",
    "        text = self.transcripts.get(file_name, \"\")\n",
    "\n",
    "        # Load audio path\n",
    "        audio_path = os.path.join(self.base_dir, f\"{file_name}.wav\")\n",
    "\n",
    "        return audio_path, text, mos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AttentionPooling(torch.nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attention = torch.nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = torch.nn.functional.softmax(self.attention(x), dim=1)  \n",
    "        return (weights * x).sum(dim=1)  \n",
    "\n",
    "\n",
    "attn_pool = AttentionPooling(embed_dim=512).to(device)  # Whisper Base uses 512-dim embeddings\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audio_paths, texts, labels = zip(*batch)\n",
    "    audios = [whisper.load_audio(path) for path in audio_paths]\n",
    "    audios = [whisper.pad_or_trim(audio) for audio in audios]\n",
    "    mel_spectrograms = [whisper.log_mel_spectrogram(audio).to(device) for audio in audios]\n",
    "    mel_spectrograms = torch.stack(mel_spectrograms)\n",
    "\n",
    "    # Compute audio embeddings with gradients enabled\n",
    "    audio_embeddings = whisper_model.encoder(mel_spectrograms).mean(dim=1)\n",
    "\n",
    "    # Process texts using BERT\n",
    "    inputs = tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = bert_model(**inputs).last_hidden_state[:, 0, :]\n",
    "\n",
    "    labels = torch.stack(labels).to(device)\n",
    "    return audio_embeddings, text_embeddings, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e84ffc-3b2c-47fd-9c75-6eff75740056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Weak Learners ---\n",
    "class WeakLearners(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, device=\"cuda:1\"):\n",
    "        super(WeakLearners, self).__init__()\n",
    "        self.audio_dim = audio_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.device = device\n",
    "\n",
    "        # Existing models\n",
    "        self.ridge_regressor = Ridge(alpha=1.0)\n",
    "        self.svr = SVR()\n",
    "        self.dtr = DecisionTreeRegressor()\n",
    "\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, train_loader):\n",
    "        \"\"\" Train weak learners using train dataset embeddings \"\"\"\n",
    "        print(\"Fitting weak learners...\")\n",
    "        all_audio_emb, all_text_emb, all_labels = [], [], []\n",
    "\n",
    "        for audio_emb, text_emb, labels in tqdm(train_loader, desc=\"Processing embeddings\", unit=\"batch\"):\n",
    "            all_audio_emb.append(audio_emb.cpu().detach().numpy())\n",
    "            all_text_emb.append(text_emb.cpu().detach().numpy())\n",
    "            all_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "        if not all_audio_emb or not all_text_emb or not all_labels:\n",
    "            raise RuntimeError(\"No embeddings found in the dataset! Check if the train_loader is correctly loading data.\")\n",
    "\n",
    "        all_audio_emb = np.vstack(all_audio_emb)\n",
    "        all_text_emb = np.vstack(all_text_emb)\n",
    "        all_labels = np.hstack(all_labels)\n",
    "\n",
    "        combined_embeddings = np.hstack((all_audio_emb, all_text_emb))\n",
    "\n",
    "        print(\"Training weak learners...\")\n",
    "        models = [\n",
    "            (self.ridge_regressor, \"Ridge Regression\"),\n",
    "            (self.svr, \"SVR\"),\n",
    "            (self.dtr, \"Decision Tree\")\n",
    "        ]\n",
    "        for model, name in models:\n",
    "            with tqdm(total=1, desc=f\"Training {name}\", unit=\"step\") as pbar:\n",
    "                model.fit(combined_embeddings, all_labels)\n",
    "                pbar.update(1)\n",
    "\n",
    "        self.fitted = True\n",
    "        print(\"Weak learners training completed.\")\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Weak learners have not been fitted. Call 'fit()' before using the model.\")\n",
    "\n",
    "        # Combine embeddings along the feature dimension\n",
    "        combined_embeddings = torch.cat([audio_emb, text_emb], dim=1).cpu().detach().numpy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ridge_pred = self.ridge_regressor.predict(combined_embeddings)\n",
    "            svr_pred = self.svr.predict(combined_embeddings)\n",
    "            dtr_pred = self.dtr.predict(combined_embeddings)\n",
    "\n",
    "        # Convert predictions to torch tensors and move to the specified device\n",
    "        ridge_pred = torch.from_numpy(ridge_pred).float().to(self.device)\n",
    "        svr_pred = torch.from_numpy(svr_pred).float().to(self.device)\n",
    "        dtr_pred = torch.from_numpy(dtr_pred).float().to(self.device)\n",
    "\n",
    "        return ridge_pred, svr_pred, dtr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ecacc4d-08b4-4ee1-8f43-6ce200e65022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced Meta-Learner ---\n",
    "class EnhancedStackingMetaLearner(nn.Module):\n",
    "    def __init__(self, weak_output_dim=3, hidden_dim=256, dropout_rate=0.1):\n",
    "        super(EnhancedStackingMetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(weak_output_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "    def forward(self, weak_outputs):\n",
    "        x = F.relu(self.bn1(self.fc1(weak_outputs)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        out = self.fc3(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423b5ecb-80b9-4e0c-a10c-86d6f40c5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Ensemble Model ---\n",
    "class SSLEnsembleModel(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, hidden_dim=256, weak_learners=None):\n",
    "        super(SSLEnsembleModel, self).__init__()\n",
    "        if weak_learners is None:\n",
    "            raise ValueError(\"Weak learners must be provided and fitted before initializing SSLEnsembleModel.\")\n",
    "        \n",
    "        self.weak_learners = weak_learners\n",
    "        # Now we have 5 weak learners' outputs\n",
    "        self.stacking_meta_learner = EnhancedStackingMetaLearner(weak_output_dim=3, hidden_dim=hidden_dim)\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        if not self.weak_learners.fitted:\n",
    "            raise RuntimeError(\"Weak learners have not been fitted. Call 'fit()' before using the model.\")\n",
    "        \n",
    "        # Get predictions from all weak learners\n",
    "        preds = self.weak_learners(audio_emb, text_emb)\n",
    "        # Stack predictions along a new dimension (batch_size x num_models)\n",
    "        weak_outputs = torch.stack(preds, dim=1)\n",
    "\n",
    "        final_output = self.stacking_meta_learner(weak_outputs)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73fc2793-3189-44df-87b4-d216b1a84d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc=\"Evaluation\", leave=False)\n",
    "        for audio_emb, text_emb, labels in test_pbar:\n",
    "            audio_emb, text_emb, labels = audio_emb.to(device), text_emb.to(device), labels.to(device)\n",
    "            outputs = model(audio_emb, text_emb)\n",
    "            preds = outputs.squeeze()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            test_pbar.set_postfix({\n",
    "                \"predicted\": [f\"{p:.2f}\" for p in preds[:5].cpu().numpy()],\n",
    "                \"ground_truth\": [f\"{l:.2f}\" for l in labels[:5].cpu().numpy()]\n",
    "            })\n",
    "\n",
    "    # Convert lists to numpy arrays for easier calculation\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Accuracy (up to ±0.5)\n",
    "    accuracy = np.mean(np.abs(all_preds - all_labels) <= 0.5)\n",
    "\n",
    "    # MSE and RMSE\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # LCC (Linear Correlation Coefficient)\n",
    "    lcc = np.corrcoef(all_labels, all_preds)[0, 1]\n",
    "\n",
    "    # KTAU (Kendall's Tau)\n",
    "    k_tau, _ = kendalltau(all_labels, all_preds)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Accuracy (±0.5): {accuracy*100:.2f}%\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"LCC: {lcc:.4f}\")\n",
    "    print(f\"KTAU: {k_tau:.4f}\")\n",
    "\n",
    "    # Show 5 examples of predicted and ground truth MOS\n",
    "    print(\"\\n5 Examples of Predicted and Ground Truth MOS:\")\n",
    "    for i in range(5):\n",
    "        print(f\"Pred: {all_preds[i]:.2f}, GT: {all_labels[i]:.2f}\")\n",
    "\n",
    "    return mse, rmse, lcc, k_tau, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "246a2b72-6b7c-46d8-b3f5-c50e495e5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Training Loop ---\n",
    "def main():\n",
    "    train_csv = \"archive/normalised_somos.csv\"\n",
    "    transcript_file = \"archive/all_transcripts.txt\"\n",
    "    \n",
    "    train_dataset = SOMOSDataset(train_csv, transcript_file, split=\"train\")\n",
    "    val_dataset = SOMOSDataset(train_csv, transcript_file, split=\"val\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    first_batch = next(iter(train_loader))\n",
    "\n",
    "    # Print batch details\n",
    "    audio_embeddings, text_embeddings, labels = first_batch\n",
    "    print(\"Audio Embeddings Shape:\", audio_embeddings.shape)  # Should be (batch_size, embed_dim)\n",
    "    print(\"Text Embeddings Shape:\", text_embeddings.shape)  # Should be (batch_size, embed_dim)\n",
    "    print(\"Labels:\", labels)  # Check if MOS labels are correctly loaded\n",
    "\n",
    "\n",
    "\n",
    "    dummy_audio, dummy_text, _ = next(iter(train_loader))\n",
    "    audio_dim = len(dummy_audio)\n",
    "    text_dim = len(dummy_text)\n",
    "    \n",
    "    weak_learners = WeakLearners(audio_dim, text_dim).to(device)\n",
    "    weak_learners.fit(train_loader)\n",
    "    \n",
    "    model = SSLEnsembleModel(audio_dim, text_dim, hidden_dim=256, weak_learners=weak_learners).to(device)\n",
    "\n",
    "    wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-6)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    num_epochs = 20\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Loop\n",
    "        model.train()\n",
    "        running_loss, total_samples = 0.0, 0\n",
    "    \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "        for audio_emb, text_emb, labels in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(audio_emb, text_emb)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            running_loss += loss.item() * audio_emb.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "    \n",
    "        train_mse = running_loss / total_samples\n",
    "        wandb.log({\"train_mse\": train_mse})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train MSE: {train_mse:.4f}\")\n",
    "    \n",
    "        # Validation\n",
    "        val_mse, val_rmse, val_lcc, val_k_tau, val_acc = evaluate(model, val_loader, device)\n",
    "        wandb.log({\"val_mse\": val_mse, \"val_rmse\": val_rmse, \"val_lcc\": val_lcc, \"val_k_tau\": val_k_tau, \"val_accuracy\": val_acc})\n",
    "    \n",
    "        if val_mse < best_mse:\n",
    "            best_mse = val_mse\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    \n",
    "    print(\"Training complete! Best validation MSE:\", best_mse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a239c90-a1d1-4757-a3d2-68f3db1d2200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Embeddings Shape: torch.Size([4, 512])\n",
      "Text Embeddings Shape: torch.Size([4, 768])\n",
      "Labels: tensor([3.4339, 3.1222, 2.8217, 4.2298], device='cuda:1')\n",
      "Fitting weak learners...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing embeddings: 100%|████████████████████████████████████████████████████| 4020/4020 [17:47<00:00,  3.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training weak learners...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Ridge Regression: 100%|███████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.39s/step]\n",
      "Training SVR: 100%|███████████████████████████████████████████████████████████████████| 1/1 [02:51<00:00, 171.85s/step]\n",
      "Training Decision Tree: 100%|██████████████████████████████████████████████████████████| 1/1 [00:32<00:00, 32.79s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak learners training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train MSE: 8.5159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 0.95%\n",
      "MSE: 5.8949\n",
      "RMSE: 2.4280\n",
      "LCC: 0.1775\n",
      "KTAU: 0.1430\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 1.02, GT: 3.48\n",
      "Pred: 0.69, GT: 3.36\n",
      "Pred: 0.89, GT: 3.40\n",
      "Pred: 0.27, GT: 3.62\n",
      "Pred: 1.22, GT: 3.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train MSE: 3.4225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 6.52%\n",
      "MSE: 2.7924\n",
      "RMSE: 1.6710\n",
      "LCC: 0.0929\n",
      "KTAU: 0.1163\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 1.96, GT: 3.48\n",
      "Pred: 1.36, GT: 3.36\n",
      "Pred: 1.59, GT: 3.40\n",
      "Pred: 0.71, GT: 3.62\n",
      "Pred: 1.70, GT: 3.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train MSE: 0.9160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 16.92%\n",
      "MSE: 1.3433\n",
      "RMSE: 1.1590\n",
      "LCC: -0.0001\n",
      "KTAU: 0.0585\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 2.45, GT: 3.48\n",
      "Pred: 2.50, GT: 3.36\n",
      "Pred: 2.39, GT: 3.40\n",
      "Pred: 1.62, GT: 3.62\n",
      "Pred: 2.07, GT: 3.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train MSE: 0.2786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 29.20%\n",
      "MSE: 0.8958\n",
      "RMSE: 0.9465\n",
      "LCC: -0.0354\n",
      "KTAU: 0.0308\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 2.54, GT: 3.48\n",
      "Pred: 2.96, GT: 3.36\n",
      "Pred: 2.91, GT: 3.40\n",
      "Pred: 1.82, GT: 3.62\n",
      "Pred: 2.37, GT: 3.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train MSE: 0.2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 31.44%\n",
      "MSE: 0.9843\n",
      "RMSE: 0.9921\n",
      "LCC: -0.1000\n",
      "KTAU: 0.0109\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 2.93, GT: 3.48\n",
      "Pred: 2.94, GT: 3.36\n",
      "Pred: 2.67, GT: 3.40\n",
      "Pred: 2.27, GT: 3.62\n",
      "Pred: 2.40, GT: 3.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train MSE: 0.2152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 36.29%\n",
      "MSE: 0.7469\n",
      "RMSE: 0.8642\n",
      "LCC: -0.0324\n",
      "KTAU: 0.0269\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 2.60, GT: 3.48\n",
      "Pred: 3.06, GT: 3.36\n",
      "Pred: 2.98, GT: 3.40\n",
      "Pred: 2.11, GT: 3.62\n",
      "Pred: 2.61, GT: 3.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train MSE: 0.2030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 33.21%\n",
      "MSE: 0.8820\n",
      "RMSE: 0.9391\n",
      "LCC: -0.0351\n",
      "KTAU: 0.0301\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 2.62, GT: 3.48\n",
      "Pred: 2.78, GT: 3.36\n",
      "Pred: 2.68, GT: 3.40\n",
      "Pred: 1.92, GT: 3.62\n",
      "Pred: 2.83, GT: 3.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train MSE: 0.1947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 35.55%\n",
      "MSE: 0.7202\n",
      "RMSE: 0.8486\n",
      "LCC: 0.0857\n",
      "KTAU: 0.1001\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 2.76, GT: 3.48\n",
      "Pred: 2.60, GT: 3.36\n",
      "Pred: 2.52, GT: 3.40\n",
      "Pred: 1.77, GT: 3.62\n",
      "Pred: 2.73, GT: 3.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training:  46%|██████████████████████▎                         | 1868/4020 [09:57<12:46,  2.81it/s, loss=0.226]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8be53f-d3ba-4604-a7c2-020dab6cd4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ccbae-827a-46f6-b571-15eb5c400df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d58aa0-3f2d-4dfb-9a98-7f25e8a52fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e3943-87a3-492e-9dbf-a19b8e5b8d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
