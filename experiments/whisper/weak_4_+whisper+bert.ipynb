{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f28df6c-fac3-47e8-8457-ee62321f061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import Wav2Vec2Model, HubertModel, WavLMModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import wandb\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from whisper import load_model\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb808b6-c3f8-4f50-9319-842844d3cffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fb6341-8550-49ee-9cbe-7f2c0b53899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Unfreeze Whisper‑medium ---\n",
    "whisper_model = whisper.load_model(\"base.en\").to(device)\n",
    "for param in whisper_model.parameters():\n",
    "    param.requires_grad = True\n",
    "whisper_model.train()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a74f2f2-029d-436d-bd96-c8c4795a0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: rtfiof (rtfiof-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>F:\\Project\\att_2\\wandb\\run-20250321_110357-s53t9l2o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl/runs/s53t9l2o' target=\"_blank\">finetune-whisper+bert</a></strong> to <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl' target=\"_blank\">https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl/runs/s53t9l2o' target=\"_blank\">https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl/runs/s53t9l2o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B online. Running your script from this directory will now sync to the cloud.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize wandb ---\n",
    "wandb.init(project=\"somos-ensemble2-ssl\", name=\"finetune-whisper+bert\")\n",
    "!wandb online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "302e5bf6-a061-469e-bd72-c1cdae6dafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "def load_json(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_audio_path(clean_path, base_dir=\"data/somos/audios\"):\n",
    "    return os.path.join(base_dir, clean_path.replace(\"\\\\\", \"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f01dabc-9bf4-4325-92c2-66e2326d6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Class ---\n",
    "class SOMOSDataset(Dataset):\n",
    "    def __init__(self, json_file, base_dir=\"data/somos/audios\"):\n",
    "        self.samples = load_json(json_file)\n",
    "        self.base_dir = base_dir\n",
    "        self.labels = [float(sample[\"mos\"]) for sample in self.samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        text = sample[\"text\"]\n",
    "        label = torch.tensor(float(sample[\"mos\"]), dtype=torch.float)\n",
    "        audio_path = process_audio_path(sample[\"clean path\"], self.base_dir)\n",
    "        return audio_path, text, label\n",
    "\n",
    "\n",
    "\n",
    "class AttentionPooling(torch.nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attention = torch.nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = torch.nn.functional.softmax(self.attention(x), dim=1)  \n",
    "        return (weights * x).sum(dim=1)  \n",
    "\n",
    "\n",
    "attn_pool = AttentionPooling(embed_dim=512).to(device)  # Whisper Base uses 512-dim embeddings\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audio_paths, texts, labels = zip(*batch)\n",
    "    audios = [whisper.load_audio(path) for path in audio_paths]\n",
    "    audios = [whisper.pad_or_trim(audio) for audio in audios]\n",
    "    mel_spectrograms = [whisper.log_mel_spectrogram(audio).to(device) for audio in audios]\n",
    "    mel_spectrograms = torch.stack(mel_spectrograms)\n",
    "\n",
    "    # Compute audio embeddings with gradients enabled\n",
    "    audio_embeddings = whisper_model.encoder(mel_spectrograms).mean(dim=1)\n",
    "\n",
    "    # Process texts using BERT\n",
    "    inputs = tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = bert_model(**inputs).last_hidden_state[:, 0, :]\n",
    "\n",
    "    labels = torch.stack(labels).to(device)\n",
    "    return audio_embeddings, text_embeddings, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e84ffc-3b2c-47fd-9c75-6eff75740056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakLearners(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, device=\"cuda:1\"):\n",
    "        super(WeakLearners, self).__init__()\n",
    "        self.audio_dim = audio_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.ridge_regressor = Ridge(alpha=1.0)\n",
    "        self.svr = SVR()\n",
    "        self.dtr = DecisionTreeRegressor()\n",
    "\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, train_loader):\n",
    "        \"\"\" Train weak learners using train dataset embeddings \"\"\"\n",
    "        print(\"Fitting weak learners...\")\n",
    "\n",
    "        all_audio_emb, all_text_emb, all_labels = [], [], []\n",
    "\n",
    "        for audio_emb, text_emb, labels in tqdm(train_loader, desc=\"Processing embeddings\", unit=\"batch\"):\n",
    "            all_audio_emb.append(audio_emb.cpu().detach().numpy())\n",
    "            all_text_emb.append(text_emb.cpu().detach().numpy())\n",
    "            all_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "        if not all_audio_emb or not all_text_emb or not all_labels:\n",
    "            raise RuntimeError(\"No embeddings found in the dataset! Check if the train_loader is correctly loading data.\")\n",
    "\n",
    "        all_audio_emb = np.vstack(all_audio_emb)\n",
    "        all_text_emb = np.vstack(all_text_emb)\n",
    "        all_labels = np.hstack(all_labels)\n",
    "\n",
    "        combined_embeddings = np.hstack((all_audio_emb, all_text_emb))\n",
    "\n",
    "        print(\"Training weak learners...\")\n",
    "        for model, name in zip([self.ridge_regressor, self.svr, self.dtr], \n",
    "                               [\"Ridge Regression\", \"SVR\", \"Decision Tree\"]):\n",
    "            with tqdm(total=1, desc=f\"Training {name}\", unit=\"step\") as pbar:\n",
    "                model.fit(combined_embeddings, all_labels)\n",
    "                pbar.update(1)\n",
    "\n",
    "        self.fitted = True\n",
    "        print(\"Weak learners training completed.\")\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Weak learners have not been fitted. Call 'fit()' before using the model.\")\n",
    "\n",
    "        combined_embeddings = torch.cat([audio_emb, text_emb], dim=1).cpu().detach().numpy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ridge_pred = self.ridge_regressor.predict(combined_embeddings)\n",
    "            svr_pred = self.svr.predict(combined_embeddings)\n",
    "            dtr_pred = self.dtr.predict(combined_embeddings)\n",
    "\n",
    "        ridge_pred = torch.from_numpy(ridge_pred).float().to(self.device)\n",
    "        svr_pred = torch.from_numpy(svr_pred).float().to(self.device)\n",
    "        dtr_pred = torch.from_numpy(dtr_pred).float().to(self.device)\n",
    "\n",
    "        return ridge_pred, svr_pred, dtr_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ecacc4d-08b4-4ee1-8f43-6ce200e65022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stacking Model (Meta-Learner) ---\n",
    "class StackingMetaLearner(nn.Module):\n",
    "    def __init__(self, weak_output_dim=3, hidden_dim=256):\n",
    "        super(StackingMetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(weak_output_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, weak_outputs):\n",
    "        x = F.relu(self.fc1(weak_outputs))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423b5ecb-80b9-4e0c-a10c-86d6f40c5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Model ---\n",
    "class SSLEnsembleModel(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, hidden_dim=256, weak_learners=None):\n",
    "        super(SSLEnsembleModel, self).__init__()\n",
    "        if weak_learners is None:\n",
    "            raise ValueError(\"Weak learners must be provided and fitted before initializing SSLEnsembleModel.\")\n",
    "        \n",
    "        self.weak_learners = weak_learners\n",
    "        self.stacking_meta_learner = StackingMetaLearner(weak_output_dim=3, hidden_dim=hidden_dim)\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        if not self.weak_learners.fitted:\n",
    "            raise RuntimeError(\"Weak learners have not been fitted. Call 'fit()' before using the model.\")\n",
    "        \n",
    "        ridge_pred, svr_pred, dtr_pred = self.weak_learners(audio_emb, text_emb)\n",
    "\n",
    "        weak_outputs = torch.stack([ridge_pred, svr_pred, dtr_pred], dim=1)\n",
    "\n",
    "        final_output = self.stacking_meta_learner(weak_outputs)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73fc2793-3189-44df-87b4-d216b1a84d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc=\"Evaluation\", leave=False)\n",
    "        for audio_emb, text_emb, labels in test_pbar:\n",
    "            audio_emb, text_emb, labels = audio_emb.to(device), text_emb.to(device), labels.to(device)\n",
    "            outputs = model(audio_emb, text_emb)\n",
    "            preds = outputs.squeeze()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            test_pbar.set_postfix({\"predicted\": preds[:5].cpu().numpy(), \"ground_truth\": labels[:5].cpu().numpy()})\n",
    "\n",
    "    # Convert lists to numpy arrays for easier calculation\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Accuracy (up to ±0.5)\n",
    "    accuracy = np.mean(np.abs(all_preds - all_labels) <= 0.5)\n",
    "\n",
    "    # MSE and RMSE\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # LCC (Linear Correlation Coefficient)\n",
    "    lcc = np.corrcoef(all_labels, all_preds)[0, 1]\n",
    "\n",
    "    # KTAU (Kendall's Tau)\n",
    "    k_tau, _ = kendalltau(all_labels, all_preds)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Accuracy (±0.5): {accuracy*100:.2f}%\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"LCC: {lcc:.4f}\")\n",
    "    print(f\"KTAU: {k_tau:.4f}\")\n",
    "\n",
    "    # Show 30 examples of predicted and ground truth MOS\n",
    "    print(\"\\n5 Examples of Predicted and Ground Truth MOS:\")\n",
    "    for i in range(5):\n",
    "        print(f\"Pred: {all_preds[i]:.2f}, GT: {all_labels[i]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "246a2b72-6b7c-46d8-b3f5-c50e495e5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Training Loop ---\n",
    "def main():\n",
    "    train_json = \"data/somos/audios/train_new.json\"\n",
    "    test_json = \"data/somos/audios/test_new.json\"\n",
    "\n",
    "    train_dataset = SOMOSDataset(train_json)\n",
    "    test_dataset = SOMOSDataset(test_json)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    dummy_audio, dummy_text, _ = next(iter(train_loader))\n",
    "    audio_dim, text_dim = dummy_audio.shape[1], dummy_text.shape[1]\n",
    "    \n",
    "    weak_learners = WeakLearners(audio_dim, text_dim).to(device)\n",
    "    weak_learners.fit(train_loader)\n",
    "    \n",
    "    model = SSLEnsembleModel(audio_dim, text_dim, hidden_dim=256, weak_learners=weak_learners).to(device)\n",
    "\n",
    "    wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    num_epochs = 20\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, total_samples = 0.0, 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "        for audio_emb, text_emb, labels in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(audio_emb, text_emb)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * audio_emb.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_mse = running_loss / total_samples\n",
    "        wandb.log({\"train_mse\": train_mse})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train MSE: {train_mse:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_loss, total_samples = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
    "            for audio_emb, text_emb, labels in test_pbar:\n",
    "                outputs = model(audio_emb, text_emb)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                test_loss += loss.item() * audio_emb.size(0)\n",
    "                total_samples += labels.size(0)\n",
    "                test_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        test_mse = test_loss / total_samples\n",
    "        wandb.log({\"val_mse\": test_mse})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Val MSE: {test_mse:.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        evaluate(model, test_loader, device)  # Call the evaluation function after each epoch\n",
    "\n",
    "        if test_mse < best_mse:\n",
    "            best_mse = test_mse\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    print(\"Training complete! Best validation MSE:\", best_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a239c90-a1d1-4757-a3d2-68f3db1d2200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting weak learners...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing embeddings: 100%|████████████████████████████████████████████████████| 3525/3525 [13:19<00:00,  4.41batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training weak learners...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Ridge Regression: 100%|███████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.83step/s]\n",
      "Training SVR: 100%|████████████████████████████████████████████████████████████████████| 1/1 [01:23<00:00, 83.14s/step]\n",
      "Training Decision Tree: 100%|██████████████████████████████████████████████████████████| 1/1 [00:19<00:00, 19.38s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak learners training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train MSE: 3.0637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Val MSE: 1.7922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 6.33%\n",
      "MSE: 1.7922\n",
      "RMSE: 1.3387\n",
      "LCC: 0.6218\n",
      "KTAU: 0.4352\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 2.28, GT: 4.00\n",
      "Pred: 2.40, GT: 4.00\n",
      "Pred: 2.26, GT: 3.73\n",
      "Pred: 2.17, GT: 3.40\n",
      "Pred: 2.01, GT: 3.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train MSE: 0.9554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Val MSE: 0.4727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 44.73%\n",
      "MSE: 0.4727\n",
      "RMSE: 0.6875\n",
      "LCC: 0.6089\n",
      "KTAU: 0.4234\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 3.15, GT: 4.00\n",
      "Pred: 3.31, GT: 4.00\n",
      "Pred: 3.15, GT: 3.73\n",
      "Pred: 3.03, GT: 3.40\n",
      "Pred: 2.80, GT: 3.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train MSE: 0.1894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Val MSE: 0.2090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 72.70%\n",
      "MSE: 0.2090\n",
      "RMSE: 0.4572\n",
      "LCC: 0.5992\n",
      "KTAU: 0.4148\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 3.70, GT: 4.00\n",
      "Pred: 3.88, GT: 4.00\n",
      "Pred: 3.72, GT: 3.73\n",
      "Pred: 3.59, GT: 3.40\n",
      "Pred: 3.30, GT: 3.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train MSE: 0.0936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Val MSE: 0.2113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (±0.5): 73.33%\n",
      "MSE: 0.2113\n",
      "RMSE: 0.4597\n",
      "LCC: 0.5883\n",
      "KTAU: 0.4056\n",
      "\n",
      "5 Examples of Predicted and Ground Truth MOS:\n",
      "Pred: 3.80, GT: 4.00\n",
      "Pred: 3.97, GT: 4.00\n",
      "Pred: 3.83, GT: 3.73\n",
      "Pred: 3.70, GT: 3.40\n",
      "Pred: 3.40, GT: 3.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m running_loss, total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     33\u001b[0m train_pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 34\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maudio_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_pbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_emb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 36\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[0;32m     35\u001b[0m     audio_paths, texts, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m---> 36\u001b[0m     audios \u001b[38;5;241m=\u001b[39m [\u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m audio_paths]\n\u001b[0;32m     37\u001b[0m     audios \u001b[38;5;241m=\u001b[39m [whisper\u001b[38;5;241m.\u001b[39mpad_or_trim(audio) \u001b[38;5;28;01mfor\u001b[39;00m audio \u001b[38;5;129;01min\u001b[39;00m audios]\n\u001b[0;32m     38\u001b[0m     mel_spectrograms \u001b[38;5;241m=\u001b[39m [whisper\u001b[38;5;241m.\u001b[39mlog_mel_spectrogram(audio)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m audio \u001b[38;5;129;01min\u001b[39;00m audios]\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\site-packages\\whisper\\audio.py:58\u001b[0m, in \u001b[0;36mload_audio\u001b[1;34m(file, sr)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\subprocess.py:1628\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;66;03m# threads remain reading and the fds left open in case the user\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;66;03m# calls communicate again.\u001b[39;00m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1628\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remaining_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m   1630\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, orig_timeout)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\threading.py:1149\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\threading.py:1169\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1170\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8be53f-d3ba-4604-a7c2-020dab6cd4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ccbae-827a-46f6-b571-15eb5c400df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d58aa0-3f2d-4dfb-9a98-7f25e8a52fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e3943-87a3-492e-9dbf-a19b8e5b8d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
