{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f28df6c-fac3-47e8-8457-ee62321f061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import whisper\n",
    "import wandb\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb808b6-c3f8-4f50-9319-842844d3cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fb6341-8550-49ee-9cbe-7f2c0b53899f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 512)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load and Unfreeze Whisper‑base ---\n",
    "whisper_model = whisper.load_model(\"base\").to(device)\n",
    "for param in whisper_model.parameters():\n",
    "    param.requires_grad = True\n",
    "whisper_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba1d583-4c16-49cd-8438-87970d2e259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# --- Load Russian BERT ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "bert_model = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a74f2f2-029d-436d-bd96-c8c4795a0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: rtfiof (rtfiof-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff49c65309c4ea8a9e49b40282bb814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>F:\\Project\\att_2\\wandb\\run-20250404_235004-odf2f93y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs/runs/odf2f93y' target=\"_blank\">finetune-whisper_b+ruBERT+sbs</a></strong> to <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs' target=\"_blank\">https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs/runs/odf2f93y' target=\"_blank\">https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs/runs/odf2f93y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B online. Running your script from this directory will now sync to the cloud.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize wandb ---\n",
    "wandb.init(project=\"somos-ensemble2-ssl-sbs\", name=\"finetune-whisper_b+ruBERT+sbs\")\n",
    "!wandb online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8be53f-d3ba-4604-a7c2-020dab6cd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Russian SBS Dataset Class ---\n",
    "class RussianSBSDataset(Dataset):\n",
    "    def __init__(self, df, base_dir, subset=False):\n",
    "        \"\"\"\n",
    "        df: A pandas DataFrame containing the data.\n",
    "        base_dir: Base directory for audio files.\n",
    "        subset: If True, only a fraction of the data is used.\n",
    "        \n",
    "        Assumes the DataFrame has the following columns:\n",
    "         - wav_path: path to the high-quality audio (always better)\n",
    "         - gen_wav_path: path to the generated or lower-quality audio\n",
    "         - txt: text corresponding to the audio sample\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        if subset:\n",
    "            self.df = self.df.sample(frac=0.05, random_state=42).reset_index(drop=True)\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # For each row, the first audio is the original (better) one and the second is the generated version.\n",
    "        audio1_path = os.path.join(self.base_dir, row[\"wav_path\"])\n",
    "        audio2_path = os.path.join(self.base_dir, row[\"gen_wav_path\"])\n",
    "        \n",
    "        # Use the provided text (strip extra spaces or newline characters)\n",
    "        text = row[\"txt\"].strip()\n",
    "        \n",
    "        # Since wav_path is always better, we assign fixed SBS scores:\n",
    "        # For example: 1.0 for the original, 0.0 for the generated.\n",
    "        sbs1 = 1.0\n",
    "        sbs2 = 0.0\n",
    "        \n",
    "        return audio1_path, audio2_path, text, sbs1, sbs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c739af4e-420c-436e-9ec2-0f0b62a5c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Collate Function for SBS ---\n",
    "def collate_fn_sbs(batch):\n",
    "    audio1_paths, audio2_paths, texts, sbs1_list, sbs2_list = zip(*batch)\n",
    "    \n",
    "    # Process first audio of each pair.\n",
    "    audios1 = [whisper.load_audio(path) for path in audio1_paths]\n",
    "    audios1 = [whisper.pad_or_trim(audio) for audio in audios1]\n",
    "    mels1 = [whisper.log_mel_spectrogram(audio).to(device) for audio in audios1]\n",
    "    mels1 = torch.stack(mels1)\n",
    "    # Get audio embeddings (mean-pooled over time).\n",
    "    audio1_emb = whisper_model.encoder(mels1).mean(dim=1)\n",
    "    \n",
    "    # Process second audio of each pair.\n",
    "    audios2 = [whisper.load_audio(path) for path in audio2_paths]\n",
    "    audios2 = [whisper.pad_or_trim(audio) for audio in audios2]\n",
    "    mels2 = [whisper.log_mel_spectrogram(audio).to(device) for audio in audios2]\n",
    "    mels2 = torch.stack(mels2)\n",
    "    audio2_emb = whisper_model.encoder(mels2).mean(dim=1)\n",
    "    \n",
    "    # Process the text once per pair.\n",
    "    inputs = tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        text_emb = bert_model(**inputs).last_hidden_state[:, 0, :]\n",
    "        \n",
    "    sbs1_tensor = torch.tensor(sbs1_list, dtype=torch.float).to(device)\n",
    "    sbs2_tensor = torch.tensor(sbs2_list, dtype=torch.float).to(device)\n",
    "    \n",
    "    # Return the embeddings along with the original file paths.\n",
    "    return audio1_emb, audio2_emb, text_emb, sbs1_tensor, sbs2_tensor, audio1_paths, audio2_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5bea04-6474-410e-83ee-e22349e87cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Weak Learners (same as before) ---\n",
    "class WeakLearners(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, device=\"cuda\"):\n",
    "        super(WeakLearners, self).__init__()\n",
    "        self.audio_dim = audio_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.ridge_regressor = Ridge(alpha=1.0)\n",
    "        self.svr = SVR()\n",
    "        self.dtr = DecisionTreeRegressor()\n",
    "\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, train_loader):\n",
    "        print(\"Fitting weak learners on SBS data...\")\n",
    "        all_audio_emb, all_text_emb, all_labels = [], [], []\n",
    "        # For each pair in the batch, treat the first and second audio separately.\n",
    "        for audio1_emb, audio2_emb, text_emb, sbs1, sbs2, _, _ in tqdm(train_loader, desc=\"Extracting embeddings\", unit=\"batch\"):\n",
    "            audio1_np = audio1_emb.cpu().detach().numpy()\n",
    "            audio2_np = audio2_emb.cpu().detach().numpy()\n",
    "            text_np = text_emb.cpu().detach().numpy()\n",
    "            sbs1_np = sbs1.cpu().detach().numpy()\n",
    "            sbs2_np = sbs2.cpu().detach().numpy()\n",
    "            \n",
    "            # Append first audio example.\n",
    "            all_audio_emb.append(audio1_np)\n",
    "            all_text_emb.append(text_np)\n",
    "            all_labels.append(sbs1_np)\n",
    "            \n",
    "            # Append second audio example.\n",
    "            all_audio_emb.append(audio2_np)\n",
    "            all_text_emb.append(text_np)\n",
    "            all_labels.append(sbs2_np)\n",
    "        \n",
    "        all_audio_emb = np.vstack(all_audio_emb)\n",
    "        all_text_emb = np.vstack(all_text_emb)\n",
    "        all_labels = np.hstack(all_labels)\n",
    "        \n",
    "        # Combine audio and text embeddings.\n",
    "        combined_embeddings = np.hstack((all_audio_emb, all_text_emb))\n",
    "        \n",
    "        # Train each weak learner.\n",
    "        for model, name in zip([self.ridge_regressor, self.svr, self.dtr],\n",
    "                               [\"Ridge Regression\", \"SVR\", \"Decision Tree\"]):\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(combined_embeddings, all_labels)\n",
    "        self.fitted = True\n",
    "        print(\"Weak learners training completed.\")\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Weak learners have not been fitted. Call 'fit()' before using the model.\")\n",
    "        # Concatenate audio and text embeddings.\n",
    "        combined = torch.cat([audio_emb, text_emb], dim=1).cpu().detach().numpy()\n",
    "        with torch.no_grad():\n",
    "            ridge_pred = self.ridge_regressor.predict(combined)\n",
    "            svr_pred = self.svr.predict(combined)\n",
    "            dtr_pred = self.dtr.predict(combined)\n",
    "        # Convert predictions to tensors.\n",
    "        ridge_pred = torch.from_numpy(ridge_pred).float().to(self.device)\n",
    "        svr_pred = torch.from_numpy(svr_pred).float().to(self.device)\n",
    "        dtr_pred = torch.from_numpy(dtr_pred).float().to(self.device)\n",
    "        return ridge_pred, svr_pred, dtr_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2989e82a-737e-487d-b61e-e54711e6a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Stacking Meta-Learner ---\n",
    "class StackingMetaLearner(nn.Module):\n",
    "    def __init__(self, weak_output_dim=3, hidden_dim=256):\n",
    "        super(StackingMetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(weak_output_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, weak_outputs):\n",
    "        x = F.relu(self.fc1(weak_outputs))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536b769d-7464-46ac-af99-47d588a7a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SSLEnsembleModel (Ensemble using weak learners and meta-learner) ---\n",
    "class SSLEnsembleModel(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, hidden_dim, weak_learners):\n",
    "        super(SSLEnsembleModel, self).__init__()\n",
    "        if weak_learners is None:\n",
    "            raise ValueError(\"Weak learners must be provided and fitted before initializing SSLEnsembleModel.\")\n",
    "        self.weak_learners = weak_learners\n",
    "        self.stacking_meta_learner = StackingMetaLearner(weak_output_dim=3, hidden_dim=hidden_dim)\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        if not self.weak_learners.fitted:\n",
    "            raise RuntimeError(\"Weak learners have not been fitted. Call 'fit()' before using the model.\")\n",
    "        # Get predictions from the weak learners.\n",
    "        ridge_pred, svr_pred, dtr_pred = self.weak_learners(audio_emb, text_emb)\n",
    "        # Stack the predictions into one tensor.\n",
    "        weak_outputs = torch.stack([ridge_pred, svr_pred, dtr_pred], dim=1)\n",
    "        # Meta-learner produces the final output.\n",
    "        final_output = self.stacking_meta_learner(weak_outputs)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9695effb-a8c8-4291-9e94-6de6e9b36637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Function with Intermediate Evaluation ---\n",
    "def train_meta_learner(train_loader, test_loader, ensemble_model, optimizer, criterion, epochs=20, eval_interval=15000):\n",
    "    ensemble_model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_mse = 0.0\n",
    "        total_mae = 0.0\n",
    "        batch_count = 0\n",
    "\n",
    "        # for batch_idx, (audio1_emb, audio2_emb, text_emb, sbs1, sbs2) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        for batch_idx, (audio1_emb, audio2_emb, text_emb, sbs1, sbs2, _, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for both audio inputs.\n",
    "            pred1 = ensemble_model(audio1_emb, text_emb).squeeze()\n",
    "            pred2 = ensemble_model(audio2_emb, text_emb).squeeze()\n",
    "\n",
    "            # Compute loss\n",
    "            loss1 = criterion(pred1, sbs1)\n",
    "            loss2 = criterion(pred2, sbs2)\n",
    "            loss = loss1 + loss2\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "            # Convert to CPU for logging\n",
    "            pred1_cpu = np.atleast_1d(pred1.detach().cpu().numpy())\n",
    "            pred2_cpu = np.atleast_1d(pred2.detach().cpu().numpy())\n",
    "            sbs1_cpu = np.atleast_1d(sbs1.cpu().numpy())\n",
    "            sbs2_cpu = np.atleast_1d(sbs2.cpu().numpy())\n",
    "\n",
    "            # Compute evaluation metrics\n",
    "            total_mse += (mean_squared_error(sbs1_cpu, pred1_cpu) + mean_squared_error(sbs2_cpu, pred2_cpu))\n",
    "            total_mae += (mean_absolute_error(sbs1_cpu, pred1_cpu) + mean_absolute_error(sbs2_cpu, pred2_cpu))\n",
    "\n",
    "            if (batch_idx + 1) % eval_interval == 0:\n",
    "                print(f\"Evaluating at batch {batch_idx+1}...\")\n",
    "                evaluate(test_loader, ensemble_model, criterion)\n",
    "\n",
    "        avg_loss = total_loss / (2 * batch_count)\n",
    "        avg_mse = total_mse / (2 * batch_count)\n",
    "        avg_mae = total_mae / (2 * batch_count)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, MSE={avg_mse:.4f}, MAE={avg_mae:.4f}\")\n",
    "        wandb.log({\"epoch_loss\": avg_loss, \"epoch_mse\": avg_mse, \"epoch_mae\": avg_mae})\n",
    "        evaluate(test_loader, ensemble_model, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dda6fce-1cf6-4d71-b726-8080b1e82fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Function ---\n",
    "def evaluate(test_loader, ensemble_model, criterion):\n",
    "    ensemble_model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_mae = 0.0\n",
    "    correct_order = 0\n",
    "    total_samples = 0\n",
    "    batch_count = 0\n",
    "    better_audio_paths = []  # to store the paths of the \"better\" audio\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for audio1_emb, audio2_emb, text_emb, sbs1, sbs2, audio1_paths, audio2_paths in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            pred1 = ensemble_model(audio1_emb, text_emb).squeeze()\n",
    "            pred2 = ensemble_model(audio2_emb, text_emb).squeeze()\n",
    "\n",
    "            loss1 = criterion(pred1, sbs1)\n",
    "            loss2 = criterion(pred2, sbs2)\n",
    "            total_loss += (loss1.item() + loss2.item())\n",
    "            batch_count += 1\n",
    "\n",
    "            # Convert tensors to CPU numpy arrays for metric computation.\n",
    "            pred1_cpu = pred1.cpu().numpy()\n",
    "            pred2_cpu = pred2.cpu().numpy()\n",
    "            sbs1_cpu = sbs1.cpu().numpy()\n",
    "            sbs2_cpu = sbs2.cpu().numpy()\n",
    "\n",
    "            total_mse += (mean_squared_error(sbs1_cpu, pred1_cpu) + mean_squared_error(sbs2_cpu, pred2_cpu))\n",
    "            total_mae += (mean_absolute_error(sbs1_cpu, pred1_cpu) + mean_absolute_error(sbs2_cpu, pred2_cpu))\n",
    "\n",
    "            # Ranking: since sbs1 should be greater than sbs2, count correct order predictions.\n",
    "            correct_order += np.sum((sbs1_cpu > sbs2_cpu) == (pred1_cpu > pred2_cpu))\n",
    "            total_samples += len(sbs1_cpu)\n",
    "\n",
    "            # Determine which audio is considered \"better\" (i.e. higher predicted SBS) for each pair.\n",
    "            for i in range(len(sbs1_cpu)):\n",
    "                if pred1_cpu[i] > pred2_cpu[i]:\n",
    "                    better_audio_paths.append(audio1_paths[i])\n",
    "                else:\n",
    "                    better_audio_paths.append(audio2_paths[i])\n",
    "\n",
    "    avg_loss = total_loss / (2 * batch_count)\n",
    "    avg_mse = total_mse / (2 * batch_count)\n",
    "    avg_mae = total_mae / (2 * batch_count)\n",
    "    accuracy = correct_order / total_samples if total_samples > 0 else 0\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test MSE: {avg_mse:.4f}, Test MAE: {avg_mae:.4f}, Ranking Accuracy: {accuracy:.4f}\")\n",
    "    wandb.log({\n",
    "        \"test_loss\": avg_loss, \n",
    "        \"test_mse\": avg_mse, \n",
    "        \"test_mae\": avg_mae, \n",
    "        \"test_ranking_accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "    # Print 10 sample paths of the audio the model considers \"better\"\n",
    "    print(\"\\nSample audio paths considered 'better':\")\n",
    "    for path in better_audio_paths[:10]:\n",
    "        print(path)\n",
    "    \n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb794d82-5527-41dd-996b-6850cfd84b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting weak learners on SBS data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|████████████████████████████████████████████████████| 1550/1550 [10:57<00:00,  2.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge Regression...\n",
      "Training SVR...\n",
      "Training Decision Tree...\n",
      "Weak learners training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████| 1550/1550 [13:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.1774, MSE=0.1774, MAE=0.3066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:16<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0693, Test MSE: 0.0693, Test MAE: 0.2060, Ranking Accuracy: 0.9884\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████| 1550/1550 [13:01<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.0174, MSE=0.0174, MAE=0.1048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0666, Test MSE: 0.0666, Test MAE: 0.1664, Ranking Accuracy: 0.9787\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████| 1550/1550 [13:01<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.0085, MSE=0.0085, MAE=0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:13<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0792, Test MSE: 0.0792, Test MAE: 0.1648, Ranking Accuracy: 0.9748\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████| 1550/1550 [13:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.0043, MSE=0.0043, MAE=0.0506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:13<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0966, Test MSE: 0.0966, Test MAE: 0.1623, Ranking Accuracy: 0.9748\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████| 1550/1550 [12:57<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.0017, MSE=0.0017, MAE=0.0301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:11<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1170, Test MSE: 0.1170, Test MAE: 0.1617, Ranking Accuracy: 0.9748\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████| 1550/1550 [12:51<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.0005, MSE=0.0005, MAE=0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:11<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1340, Test MSE: 0.1340, Test MAE: 0.1630, Ranking Accuracy: 0.9729\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|█████████████████████████████████████████████████████████████████████| 1550/1550 [12:54<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=0.0002, MSE=0.0002, MAE=0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:13<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1441, Test MSE: 0.1441, Test MAE: 0.1650, Ranking Accuracy: 0.9703\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|█████████████████████████████████████████████████████████████████████| 1550/1550 [13:02<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=0.0001, MSE=0.0001, MAE=0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:14<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1489, Test MSE: 0.1489, Test MAE: 0.1656, Ranking Accuracy: 0.9697\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|█████████████████████████████████████████████████████████████████████| 1550/1550 [12:59<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=0.0000, MSE=0.0000, MAE=0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:14<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1520, Test MSE: 0.1520, Test MAE: 0.1668, Ranking Accuracy: 0.9690\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [13:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.0000, MSE=0.0000, MAE=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:13<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1538, Test MSE: 0.1538, Test MAE: 0.1668, Ranking Accuracy: 0.9690\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [13:01<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=0.0000, MSE=0.0000, MAE=0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:13<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1560, Test MSE: 0.1560, Test MAE: 0.1676, Ranking Accuracy: 0.9677\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [12:56<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=0.0000, MSE=0.0000, MAE=0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:12<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1579, Test MSE: 0.1579, Test MAE: 0.1685, Ranking Accuracy: 0.9665\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [12:55<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.0000, MSE=0.0000, MAE=0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:11<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1595, Test MSE: 0.1595, Test MAE: 0.1692, Ranking Accuracy: 0.9658\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [12:53<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=0.0000, MSE=0.0000, MAE=0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:14<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1604, Test MSE: 0.1604, Test MAE: 0.1694, Ranking Accuracy: 0.9632\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [12:53<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=0.0000, MSE=0.0000, MAE=0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:11<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1617, Test MSE: 0.1617, Test MAE: 0.1700, Ranking Accuracy: 0.9600\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [12:55<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss=0.0000, MSE=0.0000, MAE=0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:10<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1625, Test MSE: 0.1625, Test MAE: 0.1703, Ranking Accuracy: 0.9542\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [12:55<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss=0.0000, MSE=0.0000, MAE=0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:14<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1631, Test MSE: 0.1631, Test MAE: 0.1705, Ranking Accuracy: 0.9497\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [12:57<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss=0.0000, MSE=0.0000, MAE=0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:10<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1635, Test MSE: 0.1635, Test MAE: 0.1707, Ranking Accuracy: 0.9490\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [12:45<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss=0.0000, MSE=0.0000, MAE=0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:09<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1645, Test MSE: 0.1645, Test MAE: 0.1713, Ranking Accuracy: 0.9394\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|████████████████████████████████████████████████████████████████████| 1550/1550 [12:44<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss=0.0000, MSE=0.0000, MAE=0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:09<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1646, Test MSE: 0.1646, Test MAE: 0.1712, Ranking Accuracy: 0.9426\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 388/388 [03:09<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1646, Test MSE: 0.1646, Test MAE: 0.1712, Ranking Accuracy: 0.9426\n",
      "\n",
      "Sample audio paths considered 'better':\n",
      "./buriy_audiobooks_2_val/3/c3/eaa7f742fe0f.wav\n",
      "./buriy_audiobooks_2_val/b/e9/aa5506a728f1.wav\n",
      "./buriy_audiobooks_2_val/a/09/9146542460f7.wav\n",
      "./buriy_audiobooks_2_val/6/43/0a22b6d2ae6b.wav\n",
      "./buriy_audiobooks_2_val/6/60/4372d2481049.wav\n",
      "./buriy_audiobooks_2_val/1/29/1b138a36c183.wav\n",
      "./buriy_audiobooks_2_val/c/b4/a9cf4621c185.wav\n",
      "./buriy_audiobooks_2_val/2/fd/40e353ba9921.wav\n",
      "./buriy_audiobooks_2_val/4/fe/2a8ca63e231f.wav\n",
      "./buriy_audiobooks_2_val/5/83/7847bd102962.wav\n"
     ]
    }
   ],
   "source": [
    "# --- Main Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Base directory for audio files (adjust as needed)\n",
    "    base_audio_dir = \"./\"  # or a specific path\n",
    "\n",
    "    # Load the single CSV file.\n",
    "    csv_path = \"buriy_audiobooks_2_val/df_gen_xtts_2.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Perform an 80/20 train-test split.\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create dataset objects.\n",
    "    # subset = True  # set to False for full dataset\n",
    "    subset = False\n",
    "    \n",
    "    train_dataset = RussianSBSDataset(train_df, base_dir=base_audio_dir, subset=subset)\n",
    "    test_dataset = RussianSBSDataset(test_df, base_dir=base_audio_dir, subset=subset)\n",
    "    \n",
    "    # Create dataloaders.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn_sbs)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn_sbs)\n",
    "    \n",
    "    # Initialize and fit weak learners.\n",
    "    weak_learners = WeakLearners(audio_dim=512, text_dim=768, device=device)\n",
    "    weak_learners.fit(train_loader)\n",
    "    \n",
    "    # Initialize the ensemble model.\n",
    "    ensemble_model = SSLEnsembleModel(audio_dim=512, text_dim=768, hidden_dim=256, weak_learners=weak_learners).to(device)\n",
    "    \n",
    "    # Train the stacking meta-learner.\n",
    "    optimizer = torch.optim.Adam(ensemble_model.stacking_meta_learner.parameters(), lr=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "    train_meta_learner(train_loader, test_loader, ensemble_model, optimizer, criterion, epochs=20)\n",
    "    \n",
    "    # Final evaluation on the test set.\n",
    "    evaluate(test_loader, ensemble_model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8af55-7275-48db-b4d6-cfcd5851a681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759aeb36-d489-4a14-96c4-2015cd563c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b50520-d734-48e1-8b86-4df4c79f6829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a3078-8847-47fb-bd64-ed3b60f73cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ccbae-827a-46f6-b571-15eb5c400df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d58aa0-3f2d-4dfb-9a98-7f25e8a52fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e3943-87a3-492e-9dbf-a19b8e5b8d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
