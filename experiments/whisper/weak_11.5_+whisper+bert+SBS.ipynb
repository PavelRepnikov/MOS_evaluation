{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f28df6c-fac3-47e8-8457-ee62321f061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import whisper\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb808b6-c3f8-4f50-9319-842844d3cffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fb6341-8550-49ee-9cbe-7f2c0b53899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Unfreeze Whisper‑medium and BERT ---\n",
    "whisper_model = whisper.load_model(\"base.en\").to(device)\n",
    "for param in whisper_model.parameters():\n",
    "    param.requires_grad = True\n",
    "whisper_model.train()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a74f2f2-029d-436d-bd96-c8c4795a0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: rtfiof (rtfiof-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d50499878a440338a824ba79560ce38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01128888888957186, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>F:\\Project\\att_2\\wandb\\run-20250404_134601-hzdkmaue</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs/runs/hzdkmaue' target=\"_blank\">finetune-whisper_b+bert+sbs</a></strong> to <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs' target=\"_blank\">https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs/runs/hzdkmaue' target=\"_blank\">https://wandb.ai/rtfiof-hse-university/somos-ensemble2-ssl-sbs/runs/hzdkmaue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B online. Running your script from this directory will now sync to the cloud.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize wandb ---\n",
    "wandb.init(project=\"somos-ensemble2-ssl-sbs\", name=\"finetune-whisper_b+bert+sbs\")\n",
    "!wandb online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f8be53f-d3ba-4604-a7c2-020dab6cd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SBS Dataset with Optional Subsetting ---\n",
    "class SBSDataset(Dataset):\n",
    "    def __init__(self, csv_file, base_dir, subset=False, is_test=False):\n",
    "        \"\"\"\n",
    "        csv_file: Path to train or test CSV file.\n",
    "        base_dir: Base directory for audio files.\n",
    "        subset: If True, only 0.1% of the data is used.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "\n",
    "        if subset:\n",
    "            self.df = self.df.sample(frac=0.05, random_state=42).reset_index(drop=True)\n",
    "            # self.df = self.df.sample(frac=0.001, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        self.base_dir = base_dir\n",
    "        self.is_test = is_test\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        pair = row[\"utterance_pairs\"]\n",
    "        audio1_name, audio2_name = pair.split(\"+\")\n",
    "        audio1_path = os.path.join(self.base_dir, audio1_name)\n",
    "        audio2_path = os.path.join(self.base_dir, audio2_name)\n",
    "        \n",
    "        text_column = \"right_text\" if not self.is_test else \"text\"\n",
    "        text = row[text_column]\n",
    "        \n",
    "        sbs1 = float(row[\"SBS_1\"])\n",
    "        sbs2 = float(row[\"SBS_2\"])\n",
    "        return audio1_path, audio2_path, text, sbs1, sbs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c739af4e-420c-436e-9ec2-0f0b62a5c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Collate Function for SBS ---\n",
    "def collate_fn_sbs(batch):\n",
    "    audio1_paths, audio2_paths, texts, sbs1_list, sbs2_list = zip(*batch)\n",
    "    \n",
    "    # Process first audio of each pair.\n",
    "    audios1 = [whisper.load_audio(path) for path in audio1_paths]\n",
    "    audios1 = [whisper.pad_or_trim(audio) for audio in audios1]\n",
    "    mels1 = [whisper.log_mel_spectrogram(audio).to(device) for audio in audios1]\n",
    "    mels1 = torch.stack(mels1)\n",
    "    # Get audio embeddings (mean-pooled over time).\n",
    "    audio1_emb = whisper_model.encoder(mels1).mean(dim=1)\n",
    "    \n",
    "    # Process second audio of each pair.\n",
    "    audios2 = [whisper.load_audio(path) for path in audio2_paths]\n",
    "    audios2 = [whisper.pad_or_trim(audio) for audio in audios2]\n",
    "    mels2 = [whisper.log_mel_spectrogram(audio).to(device) for audio in audios2]\n",
    "    mels2 = torch.stack(mels2)\n",
    "    audio2_emb = whisper_model.encoder(mels2).mean(dim=1)\n",
    "    \n",
    "    # Process the text once per pair.\n",
    "    inputs = tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        text_emb = bert_model(**inputs).last_hidden_state[:, 0, :]\n",
    "        \n",
    "    sbs1_tensor = torch.tensor(sbs1_list, dtype=torch.float).to(device)\n",
    "    sbs2_tensor = torch.tensor(sbs2_list, dtype=torch.float).to(device)\n",
    "    \n",
    "    return audio1_emb, audio2_emb, text_emb, sbs1_tensor, sbs2_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5bea04-6474-410e-83ee-e22349e87cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Weak Learners (same as before) ---\n",
    "class WeakLearners(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, device=\"cuda\"):\n",
    "        super(WeakLearners, self).__init__()\n",
    "        self.audio_dim = audio_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.ridge_regressor = Ridge(alpha=1.0)\n",
    "        self.svr = SVR()\n",
    "        self.dtr = DecisionTreeRegressor()\n",
    "\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, train_loader):\n",
    "        print(\"Fitting weak learners on SBS data...\")\n",
    "        all_audio_emb, all_text_emb, all_labels = [], [], []\n",
    "        # For each pair in the batch, treat the first and second audio separately.\n",
    "        for audio1_emb, audio2_emb, text_emb, sbs1, sbs2 in tqdm(train_loader, desc=\"Extracting embeddings\", unit=\"batch\"):\n",
    "            # Convert to numpy arrays.\n",
    "            audio1_np = audio1_emb.cpu().detach().numpy()\n",
    "            audio2_np = audio2_emb.cpu().detach().numpy()\n",
    "            text_np = text_emb.cpu().detach().numpy()\n",
    "            sbs1_np = sbs1.cpu().detach().numpy()\n",
    "            sbs2_np = sbs2.cpu().detach().numpy()\n",
    "            \n",
    "            # Append first audio example.\n",
    "            all_audio_emb.append(audio1_np)\n",
    "            all_text_emb.append(text_np)\n",
    "            all_labels.append(sbs1_np)\n",
    "            \n",
    "            # Append second audio example.\n",
    "            all_audio_emb.append(audio2_np)\n",
    "            all_text_emb.append(text_np)\n",
    "            all_labels.append(sbs2_np)\n",
    "        \n",
    "        all_audio_emb = np.vstack(all_audio_emb)\n",
    "        all_text_emb = np.vstack(all_text_emb)\n",
    "        all_labels = np.hstack(all_labels)\n",
    "        \n",
    "        # Combine audio and text embeddings.\n",
    "        combined_embeddings = np.hstack((all_audio_emb, all_text_emb))\n",
    "        \n",
    "        # Train each weak learner.\n",
    "        for model, name in zip([self.ridge_regressor, self.svr, self.dtr],\n",
    "                               [\"Ridge Regression\", \"SVR\", \"Decision Tree\"]):\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(combined_embeddings, all_labels)\n",
    "        self.fitted = True\n",
    "        print(\"Weak learners training completed.\")\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Weak learners have not been fitted. Call 'fit()' before using the model.\")\n",
    "        # Concatenate audio and text embeddings.\n",
    "        combined = torch.cat([audio_emb, text_emb], dim=1).cpu().detach().numpy()\n",
    "        with torch.no_grad():\n",
    "            ridge_pred = self.ridge_regressor.predict(combined)\n",
    "            svr_pred = self.svr.predict(combined)\n",
    "            dtr_pred = self.dtr.predict(combined)\n",
    "        # Convert predictions to tensors.\n",
    "        ridge_pred = torch.from_numpy(ridge_pred).float().to(self.device)\n",
    "        svr_pred = torch.from_numpy(svr_pred).float().to(self.device)\n",
    "        dtr_pred = torch.from_numpy(dtr_pred).float().to(self.device)\n",
    "        return ridge_pred, svr_pred, dtr_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2989e82a-737e-487d-b61e-e54711e6a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stacking Meta-Learner ---\n",
    "class StackingMetaLearner(nn.Module):\n",
    "    def __init__(self, weak_output_dim=3, hidden_dim=256):\n",
    "        super(StackingMetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(weak_output_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, weak_outputs):\n",
    "        x = F.relu(self.fc1(weak_outputs))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536b769d-7464-46ac-af99-47d588a7a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SSLEnsembleModel (Ensemble using weak learners and meta-learner) ---\n",
    "class SSLEnsembleModel(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, hidden_dim, weak_learners):\n",
    "        super(SSLEnsembleModel, self).__init__()\n",
    "        if weak_learners is None:\n",
    "            raise ValueError(\"Weak learners must be provided and fitted before initializing SSLEnsembleModel.\")\n",
    "        self.weak_learners = weak_learners\n",
    "        self.stacking_meta_learner = StackingMetaLearner(weak_output_dim=3, hidden_dim=hidden_dim)\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        if not self.weak_learners.fitted:\n",
    "            raise RuntimeError(\"Weak learners have not been fitted. Call 'fit()' before using the model.\")\n",
    "        # Get predictions from the weak learners.\n",
    "        ridge_pred, svr_pred, dtr_pred = self.weak_learners(audio_emb, text_emb)\n",
    "        # Stack the predictions into one tensor.\n",
    "        weak_outputs = torch.stack([ridge_pred, svr_pred, dtr_pred], dim=1)\n",
    "        # Meta-learner produces the final output.\n",
    "        final_output = self.stacking_meta_learner(weak_outputs)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f6e7461-d6f2-49fd-b8e7-f2f5a44e207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pairwise Ranking Loss Function ---\n",
    "def ranking_loss(pred1, pred2, sbs1, sbs2, margin=1.0):\n",
    "    # target is 1 if sbs1 > sbs2, and -1 otherwise.\n",
    "    target = torch.sign(sbs1 - sbs2)\n",
    "    # The difference between predictions should reflect the sign of the target.\n",
    "    diff = pred1 - pred2\n",
    "    # Hinge loss: if the difference is less than the margin in the correct direction, incur a loss.\n",
    "    loss = torch.mean(torch.clamp(margin - diff * target, min=0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9695effb-a8c8-4291-9e94-6de6e9b36637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modified Training Function ---\n",
    "def train_meta_learner(train_loader, test_loader, ensemble_model, optimizer, mse_criterion, epochs=20, eval_interval=15000, ranking_margin=1.0):\n",
    "    ensemble_model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_mse = 0.0\n",
    "        total_rank_loss = 0.0\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_idx, (audio1_emb, audio2_emb, text_emb, sbs1, sbs2) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward passes for both audios.\n",
    "            pred1 = ensemble_model(audio1_emb, text_emb).squeeze()\n",
    "            pred2 = ensemble_model(audio2_emb, text_emb).squeeze()\n",
    "\n",
    "            # Compute standard MSE losses for each prediction.\n",
    "            mse_loss1 = mse_criterion(pred1, sbs1)\n",
    "            mse_loss2 = mse_criterion(pred2, sbs2)\n",
    "            mse_loss = mse_loss1 + mse_loss2\n",
    "\n",
    "            # Compute pairwise ranking loss to enforce ordering.\n",
    "            rnk_loss = ranking_loss(pred1, pred2, sbs1, sbs2, margin=ranking_margin)\n",
    "            \n",
    "            # Combine losses (you can adjust the weights as needed).\n",
    "            loss = mse_loss + rnk_loss\n",
    "            total_loss += loss.item()\n",
    "            total_mse += mse_loss.item()\n",
    "            total_rank_loss += rnk_loss.item()\n",
    "\n",
    "            # Backpropagation.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "            # Evaluate periodically.\n",
    "            if (batch_idx + 1) % eval_interval == 0:\n",
    "                print(f\"Evaluating at batch {batch_idx+1}...\")\n",
    "                evaluate(test_loader, ensemble_model, mse_criterion)\n",
    "\n",
    "        avg_loss = total_loss / batch_count\n",
    "        avg_mse = total_mse / batch_count\n",
    "        avg_rank_loss = total_rank_loss / batch_count\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, MSE={avg_mse:.4f}, RankingLoss={avg_rank_loss:.4f}\")\n",
    "\n",
    "        wandb.log({\"epoch_loss\": avg_loss, \"epoch_mse\": avg_mse, \"epoch_ranking_loss\": avg_rank_loss})\n",
    "        evaluate(test_loader, ensemble_model, mse_criterion)\n",
    "\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate(test_loader, ensemble_model, criterion):\n",
    "    ensemble_model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_mae = 0.0\n",
    "    correct_order = 0\n",
    "    total_samples = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for audio1_emb, audio2_emb, text_emb, sbs1, sbs2 in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            pred1 = ensemble_model(audio1_emb, text_emb).squeeze()\n",
    "            pred2 = ensemble_model(audio2_emb, text_emb).squeeze()\n",
    "\n",
    "            loss1 = criterion(pred1, sbs1)\n",
    "            loss2 = criterion(pred2, sbs2)\n",
    "            total_loss += (loss1.item() + loss2.item())\n",
    "            batch_count += 1\n",
    "\n",
    "            # Convert to CPU for logging\n",
    "            pred1_cpu = pred1.cpu().numpy()\n",
    "            pred2_cpu = pred2.cpu().numpy()\n",
    "            sbs1_cpu = sbs1.cpu().numpy()\n",
    "            sbs2_cpu = sbs2.cpu().numpy()\n",
    "\n",
    "            total_mse += (mean_squared_error(sbs1_cpu, pred1_cpu) + mean_squared_error(sbs2_cpu, pred2_cpu))\n",
    "            total_mae += (mean_absolute_error(sbs1_cpu, pred1_cpu) + mean_absolute_error(sbs2_cpu, pred2_cpu))\n",
    "\n",
    "            # Compute Ranking Accuracy (Did the model preserve the SBS order?)\n",
    "            correct_order += np.sum((sbs1_cpu > sbs2_cpu) == (pred1_cpu > pred2_cpu))\n",
    "            total_samples += len(sbs1_cpu)\n",
    "\n",
    "    avg_loss = total_loss / (2 * batch_count)\n",
    "    avg_mse = total_mse / (2 * batch_count)\n",
    "    avg_mae = total_mae / (2 * batch_count)\n",
    "    accuracy = correct_order / total_samples if total_samples > 0 else 0\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss}, Test MSE: {avg_mse}, Test MAE: {avg_mae}, Ranking Accuracy: {accuracy:.4f}\")\n",
    "    wandb.log({\"test_loss\": avg_loss, \"test_mse\": avg_mse, \"test_mae\": avg_mae, \"test_ranking_accuracy\": accuracy})\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb794d82-5527-41dd-996b-6850cfd84b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q\\AppData\\Local\\Temp\\ipykernel_45656\\808334554.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting weak learners on SBS data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|████████████████████████████████████████████████████| 5516/5516 [44:26<00:00,  2.07batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge Regression...\n",
      "Training SVR...\n",
      "Training Decision Tree...\n",
      "Weak learners training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████| 5516/5516 [51:06<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=1.2359, MSE=0.2884, RankingLoss=0.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 279/279 [02:31<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01966735485696658, Test MSE: 0.019667354864685873, Test MAE: 0.11057937748756887, Ranking Accuracy: 0.6457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████| 5516/5516 [51:28<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.7901, MSE=0.0147, RankingLoss=0.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 279/279 [02:35<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.06107231413784494, Test MSE: 0.06107231390939551, Test MAE: 0.19970894189664967, Ranking Accuracy: 0.6386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  32%|█████████████████████▋                                               | 1738/5516 [16:33<35:58,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Main Script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the base directory to prepend to the audio filenames.\n",
    "    base_audio_dir = \"archive/update_SOMOS_v2/update_SOMOS_v2/all_audios/all_wavs\"\n",
    "\n",
    "    subset = True\n",
    "    # subset = False\n",
    "    \n",
    "    # Create datasets and dataloaders for training and testing.\n",
    "    train_dataset = SBSDataset(\"archive/train_same_pairs_text.csv\", base_dir=base_audio_dir, subset=subset, is_test=False)\n",
    "    test_dataset = SBSDataset(\"archive/test_same_pairs_text.csv\", base_dir=base_audio_dir, subset=subset, is_test=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn_sbs)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn_sbs)\n",
    "    \n",
    "    # Initialize and fit weak learners.\n",
    "    weak_learners = WeakLearners(audio_dim=512, text_dim=768, device=device)\n",
    "    weak_learners.fit(train_loader)\n",
    "    \n",
    "    # Initialize the ensemble model (which uses the fitted weak learners).\n",
    "    ensemble_model = SSLEnsembleModel(audio_dim=512, text_dim=768, hidden_dim=256, weak_learners=weak_learners).to(device)\n",
    "    \n",
    "    # Train the stacking meta-learner.\n",
    "    optimizer = torch.optim.Adam(ensemble_model.stacking_meta_learner.parameters(), lr=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "    train_meta_learner(train_loader, test_loader, ensemble_model, optimizer, criterion, epochs=20)\n",
    "\n",
    "    \n",
    "    # Evaluate on the test set.\n",
    "    evaluate(test_loader, ensemble_model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8af55-7275-48db-b4d6-cfcd5851a681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759aeb36-d489-4a14-96c4-2015cd563c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b50520-d734-48e1-8b86-4df4c79f6829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a3078-8847-47fb-bd64-ed3b60f73cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ccbae-827a-46f6-b571-15eb5c400df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d58aa0-3f2d-4dfb-9a98-7f25e8a52fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e3943-87a3-492e-9dbf-a19b8e5b8d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
