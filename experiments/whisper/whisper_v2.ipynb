{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733e69cd-23f6-4870-9e8e-b7c2186b3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import whisper\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2ccb1d-58c1-4b1a-b33d-f0285558d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c52234-0171-44e5-848c-898dabd02452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whisper_model = whisper.load_model(\"base\").to(device).eval()\n",
    "whisper_model = whisper.load_model(\"medium\").to(device).eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1f8b7f-e7ee-44a4-badf-2fa4dbaa4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_audio_path(clean_path, base_dir=\"data/somos/audios\"):\n",
    "    return os.path.join(base_dir, clean_path.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "# Dataset Class\n",
    "class SOMOSDataset(Dataset):\n",
    "    def __init__(self, json_file, base_dir=\"data/somos/audios\"):\n",
    "        self.samples = load_json(json_file)\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        text = sample[\"text\"]\n",
    "        mos = int(float(sample[\"mos\"]))\n",
    "        label = torch.tensor(mos - 1, dtype=torch.long)\n",
    "\n",
    "        audio_path = process_audio_path(sample[\"clean path\"], self.base_dir)\n",
    "\n",
    "        return audio_path, text, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audio_paths, texts, labels = zip(*batch)\n",
    "\n",
    "    audios = [whisper.load_audio(path) for path in audio_paths]\n",
    "    audios = [whisper.pad_or_trim(audio) for audio in audios]\n",
    "    mel_spectrograms = [whisper.log_mel_spectrogram(audio).to(device) for audio in audios]\n",
    "    mel_spectrograms = torch.stack(mel_spectrograms)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        audio_embeddings = whisper_model.encoder(mel_spectrograms).mean(dim=1)  # Batch audio embeddings\n",
    "\n",
    "    inputs = tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = bert_model(**inputs).last_hidden_state[:, 0, :]  # Batch text embeddings\n",
    "\n",
    "    labels = torch.stack(labels).to(device)\n",
    "\n",
    "    return audio_embeddings, text_embeddings, labels\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, hidden_dim, num_classes, dropout_rate=0.05):\n",
    "        super(FusionClassifier, self).__init__()\n",
    "\n",
    "        self.audio_fc = nn.Sequential(\n",
    "            nn.Linear(audio_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(text_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        audio_feat = self.audio_fc(audio_emb)\n",
    "        text_feat = self.text_fc(text_emb)\n",
    "\n",
    "        fusion = torch.cat([audio_feat, text_feat], dim=1)\n",
    "        attn_weights = self.attention(fusion)\n",
    "        fusion = fusion * attn_weights\n",
    "\n",
    "        return self.fusion_fc(fusion)\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, epoch, best_acc, save_path=\"models\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model_path = os.path.join(save_path, f\"model_epoch_{epoch}.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    best_model_path = os.path.join(save_path, \"best_model.pth\")\n",
    "    if best_acc:\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8881b084-fe84-47f4-8977-c903d161e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def compute_class_weights(dataset, num_classes=5):\n",
    "    labels = [int(float(sample[\"mos\"])) - 1 for sample in dataset.samples]  # Convert to 0-based index\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    total_samples = len(labels)\n",
    "    class_weights = {cls: total_samples / (num_classes * count) for cls, count in class_counts.items()}\n",
    "    \n",
    "    # Convert to a tensor\n",
    "    weights = torch.tensor([class_weights[i] for i in range(num_classes)], dtype=torch.float).to(device)\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3fb4e3-9fb6-4c7b-ae3a-f19f4ca5a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def create_weighted_sampler(dataset, num_classes=5):\n",
    "    # Compute frequency of each label\n",
    "    labels = [int(float(sample[\"mos\"])) - 1 for sample in dataset.samples]\n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    # Assign weights: lower for frequent classes, higher for rare ones\n",
    "    sample_weights = [total / counts[label] for label in labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    return sampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e26c06-ea30-442a-83a1-e70251fc70eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model architecture:\n",
      " Whisper(\n",
      "  (encoder): AudioEncoder(\n",
      "    (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TextDecoder(\n",
      "    (token_embedding): Embedding(51865, 1024)\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (cross_attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "encoder AudioEncoder(\n",
      "  (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (blocks): ModuleList(\n",
      "    (0-23): 24 x ResidualAttentionBlock(\n",
      "      (attn): MultiHeadAttention(\n",
      "        (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      )\n",
      "      (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.conv1 Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "encoder.conv2 Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "encoder.blocks ModuleList(\n",
      "  (0-23): 24 x ResidualAttentionBlock(\n",
      "    (attn): MultiHeadAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "encoder.blocks.0 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.0.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.0.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.0.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.0.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.0.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.0.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.0.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.0.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.0.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.0.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.0.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.1 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.1.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.1.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.1.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.1.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.1.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.1.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.1.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.1.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.1.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.1.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.1.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.2 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.2.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.2.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.2.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.2.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.2.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.2.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.2.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.2.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.2.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.2.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.2.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.3 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.3.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.3.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.3.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.3.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.3.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.3.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.3.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.3.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.3.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.3.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.3.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.4 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.4.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.4.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.4.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.4.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.4.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.4.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.4.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.4.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.4.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.4.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.4.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.5 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.5.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.5.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.5.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.5.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.5.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.5.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.5.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.5.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.5.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.5.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.5.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.6 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.6.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.6.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.6.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.6.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.6.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.6.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.6.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.6.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.6.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.6.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.6.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.7 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.7.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.7.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.7.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.7.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.7.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.7.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.7.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.7.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.7.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.7.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.7.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.8 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.8.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.8.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.8.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.8.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.8.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.8.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.8.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.8.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.8.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.8.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.8.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.9 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.9.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.9.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.9.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.9.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.9.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.9.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.9.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.9.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.9.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.9.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.9.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.10 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.10.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.10.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.10.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.10.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.10.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.10.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.10.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.10.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.10.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.10.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.10.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.11 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.11.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.11.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.11.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.11.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.11.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.11.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.11.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.11.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.11.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.11.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.11.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.12 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.12.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.12.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.12.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.12.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.12.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.12.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.12.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.12.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.12.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.12.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.12.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.13 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.13.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.13.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.13.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.13.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.13.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.13.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.13.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.13.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.13.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.13.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.13.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.14 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.14.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.14.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.14.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.14.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.14.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.14.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.14.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.14.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.14.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.14.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.14.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.15 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.15.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.15.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.15.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.15.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.15.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.15.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.15.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.15.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.15.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.15.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.15.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.16 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.16.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.16.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.16.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.16.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.16.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.16.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.16.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.16.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.16.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.16.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.16.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.17 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.17.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.17.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.17.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.17.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.17.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.17.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.17.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.17.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.17.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.17.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.17.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.18 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.18.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.18.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.18.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.18.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.18.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.18.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.18.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.18.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.18.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.18.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.18.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.19 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.19.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.19.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.19.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.19.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.19.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.19.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.19.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.19.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.19.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.19.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.19.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.20 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.20.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.20.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.20.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.20.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.20.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.20.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.20.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.20.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.20.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.20.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.20.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.21 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.21.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.21.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.21.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.21.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.21.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.21.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.21.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.21.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.21.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.21.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.21.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.22 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.22.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.22.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.22.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.22.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.22.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.22.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.22.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.22.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.22.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.22.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.22.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.23 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.23.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.23.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.23.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.23.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.23.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.23.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.23.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.23.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.23.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.23.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.23.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.ln_post LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder TextDecoder(\n",
      "  (token_embedding): Embedding(51865, 1024)\n",
      "  (blocks): ModuleList(\n",
      "    (0-23): 24 x ResidualAttentionBlock(\n",
      "      (attn): MultiHeadAttention(\n",
      "        (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (cross_attn): MultiHeadAttention(\n",
      "        (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      )\n",
      "      (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.token_embedding Embedding(51865, 1024)\n",
      "decoder.blocks ModuleList(\n",
      "  (0-23): 24 x ResidualAttentionBlock(\n",
      "    (attn): MultiHeadAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (cross_attn): MultiHeadAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "decoder.blocks.0 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.0.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.0.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.0.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.0.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.0.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.0.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.0.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.0.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.0.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.0.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.0.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.1 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.1.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.1.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.1.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.1.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.1.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.1.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.1.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.1.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.1.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.1.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.1.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.2 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.2.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.2.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.2.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.2.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.2.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.2.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.2.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.2.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.2.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.2.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.2.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.3 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.3.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.3.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.3.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.3.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.3.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.3.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.3.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.3.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.3.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.3.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.3.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.4 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.4.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.4.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.4.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.4.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.4.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.4.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.4.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.4.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.4.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.4.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.4.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.5 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.5.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.5.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.5.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.5.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.5.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.5.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.5.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.5.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.5.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.5.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.5.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.6 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.6.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.6.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.6.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.6.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.6.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.6.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.6.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.6.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.6.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.6.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.6.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.7 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.7.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.7.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.7.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.7.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.7.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.7.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.7.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.7.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.7.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.7.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.7.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.8 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.8.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.8.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.8.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.8.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.8.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.8.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.8.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.8.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.8.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.8.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.8.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.9 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.9.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.9.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.9.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.9.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.9.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.9.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.9.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.9.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.9.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.9.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.9.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.10 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.10.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.10.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.10.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.10.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.10.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.10.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.10.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.10.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.10.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.10.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.10.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.11 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.11.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.11.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.11.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.11.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.11.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.11.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.11.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.11.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.11.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.11.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.11.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.12 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.12.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.12.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.12.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.12.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.12.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.12.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.12.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.12.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.12.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.12.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.12.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.13 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.13.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.13.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.13.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.13.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.13.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.13.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.13.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.13.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.13.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.13.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.13.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.14 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.14.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.14.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.14.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.14.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.14.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.14.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.14.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.14.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.14.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.14.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.14.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.15 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.15.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.15.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.15.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.15.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.15.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.15.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.15.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.15.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.15.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.15.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.15.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.16 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.16.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.16.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.16.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.16.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.16.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.16.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.16.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.16.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.16.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.16.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.16.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.17 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.17.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.17.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.17.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.17.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.17.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.17.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.17.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.17.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.17.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.17.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.17.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.18 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.18.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.18.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.18.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.18.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.18.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.18.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.18.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.18.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.18.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.18.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.18.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.19 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.19.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.19.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.19.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.19.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.19.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.19.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.19.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.19.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.19.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.19.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.19.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.20 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.20.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.20.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.20.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.20.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.20.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.20.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.20.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.20.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.20.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.20.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.20.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.21 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.21.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.21.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.21.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.21.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.21.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.21.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.21.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.21.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.21.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.21.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.21.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.22 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.22.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.22.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.22.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.22.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.22.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.22.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.22.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.22.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.22.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.22.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.22.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.23 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.23.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.23.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.23.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.23.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.23.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.23.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.23.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.23.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.23.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.23.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.23.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Full model architecture:\")\n",
    "for name, module in whisper_model.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "055831e0-080c-483b-b84b-0c17ce43a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in whisper_model.encoder.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b21210-7d87-417c-a810-4ad3df715ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total encoder blocks: 24\n"
     ]
    }
   ],
   "source": [
    "encoder_blocks = whisper_model.encoder.blocks\n",
    "total_blocks = len(encoder_blocks)\n",
    "print(f\"Total encoder blocks: {total_blocks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee4a6a9-fd2e-4680-b9f6-883aa65d7849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing encoder blocks with index >= 12\n"
     ]
    }
   ],
   "source": [
    "threshold = total_blocks // 2  # unfreeze blocks with index >= threshold\n",
    "print(f\"Unfreezing encoder blocks with index >= {threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d63eb76b-ff76-45cf-bea7-f534ab89ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, block in enumerate(encoder_blocks):\n",
    "    if idx >= threshold:\n",
    "        for name, param in block.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c014d7-253d-42b0-a34b-e81bdf63a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(whisper_model.encoder, \"final_layer_norm\"):\n",
    "    for param in whisper_model.encoder.final_layer_norm.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4780bac1-3959-42ad-9e2d-5617d6ceb803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unfrozen parameters in the Whisper encoder:\n",
      "blocks.12.attn.query.weight\n",
      "blocks.12.attn.query.bias\n",
      "blocks.12.attn.key.weight\n",
      "blocks.12.attn.value.weight\n",
      "blocks.12.attn.value.bias\n",
      "blocks.12.attn.out.weight\n",
      "blocks.12.attn.out.bias\n",
      "blocks.12.attn_ln.weight\n",
      "blocks.12.attn_ln.bias\n",
      "blocks.12.mlp.0.weight\n",
      "blocks.12.mlp.0.bias\n",
      "blocks.12.mlp.2.weight\n",
      "blocks.12.mlp.2.bias\n",
      "blocks.12.mlp_ln.weight\n",
      "blocks.12.mlp_ln.bias\n",
      "blocks.13.attn.query.weight\n",
      "blocks.13.attn.query.bias\n",
      "blocks.13.attn.key.weight\n",
      "blocks.13.attn.value.weight\n",
      "blocks.13.attn.value.bias\n",
      "blocks.13.attn.out.weight\n",
      "blocks.13.attn.out.bias\n",
      "blocks.13.attn_ln.weight\n",
      "blocks.13.attn_ln.bias\n",
      "blocks.13.mlp.0.weight\n",
      "blocks.13.mlp.0.bias\n",
      "blocks.13.mlp.2.weight\n",
      "blocks.13.mlp.2.bias\n",
      "blocks.13.mlp_ln.weight\n",
      "blocks.13.mlp_ln.bias\n",
      "blocks.14.attn.query.weight\n",
      "blocks.14.attn.query.bias\n",
      "blocks.14.attn.key.weight\n",
      "blocks.14.attn.value.weight\n",
      "blocks.14.attn.value.bias\n",
      "blocks.14.attn.out.weight\n",
      "blocks.14.attn.out.bias\n",
      "blocks.14.attn_ln.weight\n",
      "blocks.14.attn_ln.bias\n",
      "blocks.14.mlp.0.weight\n",
      "blocks.14.mlp.0.bias\n",
      "blocks.14.mlp.2.weight\n",
      "blocks.14.mlp.2.bias\n",
      "blocks.14.mlp_ln.weight\n",
      "blocks.14.mlp_ln.bias\n",
      "blocks.15.attn.query.weight\n",
      "blocks.15.attn.query.bias\n",
      "blocks.15.attn.key.weight\n",
      "blocks.15.attn.value.weight\n",
      "blocks.15.attn.value.bias\n",
      "blocks.15.attn.out.weight\n",
      "blocks.15.attn.out.bias\n",
      "blocks.15.attn_ln.weight\n",
      "blocks.15.attn_ln.bias\n",
      "blocks.15.mlp.0.weight\n",
      "blocks.15.mlp.0.bias\n",
      "blocks.15.mlp.2.weight\n",
      "blocks.15.mlp.2.bias\n",
      "blocks.15.mlp_ln.weight\n",
      "blocks.15.mlp_ln.bias\n",
      "blocks.16.attn.query.weight\n",
      "blocks.16.attn.query.bias\n",
      "blocks.16.attn.key.weight\n",
      "blocks.16.attn.value.weight\n",
      "blocks.16.attn.value.bias\n",
      "blocks.16.attn.out.weight\n",
      "blocks.16.attn.out.bias\n",
      "blocks.16.attn_ln.weight\n",
      "blocks.16.attn_ln.bias\n",
      "blocks.16.mlp.0.weight\n",
      "blocks.16.mlp.0.bias\n",
      "blocks.16.mlp.2.weight\n",
      "blocks.16.mlp.2.bias\n",
      "blocks.16.mlp_ln.weight\n",
      "blocks.16.mlp_ln.bias\n",
      "blocks.17.attn.query.weight\n",
      "blocks.17.attn.query.bias\n",
      "blocks.17.attn.key.weight\n",
      "blocks.17.attn.value.weight\n",
      "blocks.17.attn.value.bias\n",
      "blocks.17.attn.out.weight\n",
      "blocks.17.attn.out.bias\n",
      "blocks.17.attn_ln.weight\n",
      "blocks.17.attn_ln.bias\n",
      "blocks.17.mlp.0.weight\n",
      "blocks.17.mlp.0.bias\n",
      "blocks.17.mlp.2.weight\n",
      "blocks.17.mlp.2.bias\n",
      "blocks.17.mlp_ln.weight\n",
      "blocks.17.mlp_ln.bias\n",
      "blocks.18.attn.query.weight\n",
      "blocks.18.attn.query.bias\n",
      "blocks.18.attn.key.weight\n",
      "blocks.18.attn.value.weight\n",
      "blocks.18.attn.value.bias\n",
      "blocks.18.attn.out.weight\n",
      "blocks.18.attn.out.bias\n",
      "blocks.18.attn_ln.weight\n",
      "blocks.18.attn_ln.bias\n",
      "blocks.18.mlp.0.weight\n",
      "blocks.18.mlp.0.bias\n",
      "blocks.18.mlp.2.weight\n",
      "blocks.18.mlp.2.bias\n",
      "blocks.18.mlp_ln.weight\n",
      "blocks.18.mlp_ln.bias\n",
      "blocks.19.attn.query.weight\n",
      "blocks.19.attn.query.bias\n",
      "blocks.19.attn.key.weight\n",
      "blocks.19.attn.value.weight\n",
      "blocks.19.attn.value.bias\n",
      "blocks.19.attn.out.weight\n",
      "blocks.19.attn.out.bias\n",
      "blocks.19.attn_ln.weight\n",
      "blocks.19.attn_ln.bias\n",
      "blocks.19.mlp.0.weight\n",
      "blocks.19.mlp.0.bias\n",
      "blocks.19.mlp.2.weight\n",
      "blocks.19.mlp.2.bias\n",
      "blocks.19.mlp_ln.weight\n",
      "blocks.19.mlp_ln.bias\n",
      "blocks.20.attn.query.weight\n",
      "blocks.20.attn.query.bias\n",
      "blocks.20.attn.key.weight\n",
      "blocks.20.attn.value.weight\n",
      "blocks.20.attn.value.bias\n",
      "blocks.20.attn.out.weight\n",
      "blocks.20.attn.out.bias\n",
      "blocks.20.attn_ln.weight\n",
      "blocks.20.attn_ln.bias\n",
      "blocks.20.mlp.0.weight\n",
      "blocks.20.mlp.0.bias\n",
      "blocks.20.mlp.2.weight\n",
      "blocks.20.mlp.2.bias\n",
      "blocks.20.mlp_ln.weight\n",
      "blocks.20.mlp_ln.bias\n",
      "blocks.21.attn.query.weight\n",
      "blocks.21.attn.query.bias\n",
      "blocks.21.attn.key.weight\n",
      "blocks.21.attn.value.weight\n",
      "blocks.21.attn.value.bias\n",
      "blocks.21.attn.out.weight\n",
      "blocks.21.attn.out.bias\n",
      "blocks.21.attn_ln.weight\n",
      "blocks.21.attn_ln.bias\n",
      "blocks.21.mlp.0.weight\n",
      "blocks.21.mlp.0.bias\n",
      "blocks.21.mlp.2.weight\n",
      "blocks.21.mlp.2.bias\n",
      "blocks.21.mlp_ln.weight\n",
      "blocks.21.mlp_ln.bias\n",
      "blocks.22.attn.query.weight\n",
      "blocks.22.attn.query.bias\n",
      "blocks.22.attn.key.weight\n",
      "blocks.22.attn.value.weight\n",
      "blocks.22.attn.value.bias\n",
      "blocks.22.attn.out.weight\n",
      "blocks.22.attn.out.bias\n",
      "blocks.22.attn_ln.weight\n",
      "blocks.22.attn_ln.bias\n",
      "blocks.22.mlp.0.weight\n",
      "blocks.22.mlp.0.bias\n",
      "blocks.22.mlp.2.weight\n",
      "blocks.22.mlp.2.bias\n",
      "blocks.22.mlp_ln.weight\n",
      "blocks.22.mlp_ln.bias\n",
      "blocks.23.attn.query.weight\n",
      "blocks.23.attn.query.bias\n",
      "blocks.23.attn.key.weight\n",
      "blocks.23.attn.value.weight\n",
      "blocks.23.attn.value.bias\n",
      "blocks.23.attn.out.weight\n",
      "blocks.23.attn.out.bias\n",
      "blocks.23.attn_ln.weight\n",
      "blocks.23.attn_ln.bias\n",
      "blocks.23.mlp.0.weight\n",
      "blocks.23.mlp.0.bias\n",
      "blocks.23.mlp.2.weight\n",
      "blocks.23.mlp.2.bias\n",
      "blocks.23.mlp_ln.weight\n",
      "blocks.23.mlp_ln.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnfrozen parameters in the Whisper encoder:\")\n",
    "for name, param in whisper_model.encoder.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5f34a-f50a-4bdb-8145-295c4bb87e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34813f0-65ca-4767-866a-2146ef831fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Loss\n",
    "\n",
    "import torchsort\n",
    "\n",
    "def emd_loss(y_pred, y_true, num_classes):\n",
    "    y_pred = F.softmax(y_pred, dim=-1)\n",
    "    y_true = F.one_hot(y_true, num_classes).float()\n",
    "\n",
    "    cdf_pred = torch.cumsum(y_pred, dim=-1)\n",
    "    cdf_true = torch.cumsum(y_true, dim=-1)\n",
    "\n",
    "    return torch.mean(torch.abs(cdf_pred - cdf_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee83f81-b5b0-4c3c-8d27-54c07bf2a49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99ef3d95-e539-443a-a985-677c52a328e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_json = \"data/somos/audios/train.json\"\n",
    "    test_json = \"data/somos/audios/test.json\"\n",
    "\n",
    "    train_dataset = SOMOSDataset(train_json)\n",
    "    test_dataset = SOMOSDataset(test_json)\n",
    "\n",
    "    # class_weights = compute_class_weights(train_dataset)\n",
    "    # print(class_weights)\n",
    "\n",
    "    train_sampler = create_weighted_sampler(train_dataset)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, sampler=train_sampler, collate_fn=collate_fn)\n",
    "    \n",
    "    # train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    dummy_audio, dummy_text, _ = next(iter(train_loader))\n",
    "    audio_dim, text_dim = dummy_audio.shape[1], dummy_text.shape[1]\n",
    "    num_classes = 5\n",
    "\n",
    "    model = FusionClassifier(audio_dim, text_dim, hidden_dim=256, num_classes=num_classes).to(device)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = lambda y_pred, y_true: emd_loss(y_pred, y_true, num_classes=5)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    num_epochs = 10\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct_preds, total_samples = 0.0, 0, 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "        for audio_emb, text_emb, labels in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(audio_emb, text_emb)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * audio_emb.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_acc = 100 * correct_preds / total_samples\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {running_loss/total_samples:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "         # Evaluation\n",
    "        model.eval()\n",
    "        test_loss, correct_preds, total_samples = 0.0, 0, 0\n",
    "        test_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
    "            for audio_emb, text_emb, labels in test_pbar:\n",
    "                audio_emb = audio_emb.to(device)\n",
    "                text_emb = text_emb.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(audio_emb, text_emb)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item() * audio_emb.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct_preds += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "                test_predictions.extend(zip(labels.cpu().tolist(), preds.cpu().tolist()))\n",
    "\n",
    "                test_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        test_acc = 100 * correct_preds / total_samples\n",
    "        avg_test_loss = test_loss / total_samples\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Val Loss: {avg_test_loss:.4f} | Val Acc: {test_acc:.2f}%\")\n",
    "\n",
    "        print(\"\\nSample Predictions (Real MOS vs Predicted MOS):\")\n",
    "        for i, (real_mos, pred_mos) in enumerate(test_predictions[:20]):\n",
    "            print(f\"Example {i+1}: Real MOS = {real_mos + 1}, Predicted MOS = {pred_mos + 1}\")  # Convert back to 1-5 scale\n",
    "\n",
    "        save_model(model, epoch + 1, test_acc > best_acc)\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "\n",
    "    print(\"Training complete! Best validation accuracy:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b2b68de-f397-49a0-9c26-fa5118ea0d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q\\AppData\\Local\\Temp\\ipykernel_2708\\642206057.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 1 Training:   0%|                                                                       | 0/3525 [00:00<?, ?it/s]C:\\Users\\q\\AppData\\Local\\Temp\\ipykernel_2708\\642206057.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.2493 | Train Acc: 19.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Val Loss: 0.2056 | Val Acc: 23.67%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 3\n",
      "Example 2: Real MOS = 3, Predicted MOS = 3\n",
      "Example 3: Real MOS = 5, Predicted MOS = 3\n",
      "Example 4: Real MOS = 2, Predicted MOS = 3\n",
      "Example 5: Real MOS = 3, Predicted MOS = 3\n",
      "Example 6: Real MOS = 4, Predicted MOS = 3\n",
      "Example 7: Real MOS = 3, Predicted MOS = 3\n",
      "Example 8: Real MOS = 3, Predicted MOS = 3\n",
      "Example 9: Real MOS = 3, Predicted MOS = 3\n",
      "Example 10: Real MOS = 3, Predicted MOS = 3\n",
      "Example 11: Real MOS = 5, Predicted MOS = 3\n",
      "Example 12: Real MOS = 5, Predicted MOS = 3\n",
      "Example 13: Real MOS = 4, Predicted MOS = 3\n",
      "Example 14: Real MOS = 3, Predicted MOS = 3\n",
      "Example 15: Real MOS = 4, Predicted MOS = 3\n",
      "Example 16: Real MOS = 5, Predicted MOS = 3\n",
      "Example 17: Real MOS = 5, Predicted MOS = 3\n",
      "Example 18: Real MOS = 4, Predicted MOS = 3\n",
      "Example 19: Real MOS = 5, Predicted MOS = 3\n",
      "Example 20: Real MOS = 2, Predicted MOS = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 0.2398 | Train Acc: 20.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Val Loss: 0.2040 | Val Acc: 23.67%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 3\n",
      "Example 2: Real MOS = 3, Predicted MOS = 3\n",
      "Example 3: Real MOS = 5, Predicted MOS = 3\n",
      "Example 4: Real MOS = 2, Predicted MOS = 3\n",
      "Example 5: Real MOS = 3, Predicted MOS = 3\n",
      "Example 6: Real MOS = 4, Predicted MOS = 3\n",
      "Example 7: Real MOS = 3, Predicted MOS = 3\n",
      "Example 8: Real MOS = 3, Predicted MOS = 3\n",
      "Example 9: Real MOS = 3, Predicted MOS = 3\n",
      "Example 10: Real MOS = 3, Predicted MOS = 3\n",
      "Example 11: Real MOS = 5, Predicted MOS = 3\n",
      "Example 12: Real MOS = 5, Predicted MOS = 3\n",
      "Example 13: Real MOS = 4, Predicted MOS = 3\n",
      "Example 14: Real MOS = 3, Predicted MOS = 3\n",
      "Example 15: Real MOS = 4, Predicted MOS = 3\n",
      "Example 16: Real MOS = 5, Predicted MOS = 3\n",
      "Example 17: Real MOS = 5, Predicted MOS = 3\n",
      "Example 18: Real MOS = 4, Predicted MOS = 3\n",
      "Example 19: Real MOS = 5, Predicted MOS = 3\n",
      "Example 20: Real MOS = 2, Predicted MOS = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 0.2406 | Train Acc: 19.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Val Loss: 0.2038 | Val Acc: 23.67%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 3\n",
      "Example 2: Real MOS = 3, Predicted MOS = 3\n",
      "Example 3: Real MOS = 5, Predicted MOS = 3\n",
      "Example 4: Real MOS = 2, Predicted MOS = 3\n",
      "Example 5: Real MOS = 3, Predicted MOS = 3\n",
      "Example 6: Real MOS = 4, Predicted MOS = 3\n",
      "Example 7: Real MOS = 3, Predicted MOS = 3\n",
      "Example 8: Real MOS = 3, Predicted MOS = 3\n",
      "Example 9: Real MOS = 3, Predicted MOS = 3\n",
      "Example 10: Real MOS = 3, Predicted MOS = 3\n",
      "Example 11: Real MOS = 5, Predicted MOS = 3\n",
      "Example 12: Real MOS = 5, Predicted MOS = 3\n",
      "Example 13: Real MOS = 4, Predicted MOS = 3\n",
      "Example 14: Real MOS = 3, Predicted MOS = 3\n",
      "Example 15: Real MOS = 4, Predicted MOS = 3\n",
      "Example 16: Real MOS = 5, Predicted MOS = 3\n",
      "Example 17: Real MOS = 5, Predicted MOS = 3\n",
      "Example 18: Real MOS = 4, Predicted MOS = 3\n",
      "Example 19: Real MOS = 5, Predicted MOS = 3\n",
      "Example 20: Real MOS = 2, Predicted MOS = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 0.2410 | Train Acc: 19.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Val Loss: 0.2037 | Val Acc: 23.67%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 3\n",
      "Example 2: Real MOS = 3, Predicted MOS = 3\n",
      "Example 3: Real MOS = 5, Predicted MOS = 3\n",
      "Example 4: Real MOS = 2, Predicted MOS = 3\n",
      "Example 5: Real MOS = 3, Predicted MOS = 3\n",
      "Example 6: Real MOS = 4, Predicted MOS = 3\n",
      "Example 7: Real MOS = 3, Predicted MOS = 3\n",
      "Example 8: Real MOS = 3, Predicted MOS = 3\n",
      "Example 9: Real MOS = 3, Predicted MOS = 3\n",
      "Example 10: Real MOS = 3, Predicted MOS = 3\n",
      "Example 11: Real MOS = 5, Predicted MOS = 3\n",
      "Example 12: Real MOS = 5, Predicted MOS = 3\n",
      "Example 13: Real MOS = 4, Predicted MOS = 3\n",
      "Example 14: Real MOS = 3, Predicted MOS = 3\n",
      "Example 15: Real MOS = 4, Predicted MOS = 3\n",
      "Example 16: Real MOS = 5, Predicted MOS = 3\n",
      "Example 17: Real MOS = 5, Predicted MOS = 3\n",
      "Example 18: Real MOS = 4, Predicted MOS = 3\n",
      "Example 19: Real MOS = 5, Predicted MOS = 3\n",
      "Example 20: Real MOS = 2, Predicted MOS = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 38\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m running_loss, correct_preds, total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     37\u001b[0m train_pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 38\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maudio_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_pbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[0;32m     28\u001b[0m     audio_paths, texts, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m---> 30\u001b[0m     audios \u001b[38;5;241m=\u001b[39m [\u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m audio_paths]\n\u001b[0;32m     31\u001b[0m     audios \u001b[38;5;241m=\u001b[39m [whisper\u001b[38;5;241m.\u001b[39mpad_or_trim(audio) \u001b[38;5;28;01mfor\u001b[39;00m audio \u001b[38;5;129;01min\u001b[39;00m audios]\n\u001b[0;32m     32\u001b[0m     mel_spectrograms \u001b[38;5;241m=\u001b[39m [whisper\u001b[38;5;241m.\u001b[39mlog_mel_spectrogram(audio)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m audio \u001b[38;5;129;01min\u001b[39;00m audios]\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\site-packages\\whisper\\audio.py:58\u001b[0m, in \u001b[0;36mload_audio\u001b[1;34m(file, sr)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\subprocess.py:1628\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;66;03m# threads remain reading and the fds left open in case the user\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;66;03m# calls communicate again.\u001b[39;00m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1628\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remaining_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m   1630\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, orig_timeout)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\threading.py:1149\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda\\envs\\project\\Lib\\threading.py:1169\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1170\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3b105-a6ad-4d79-98c9-2c531a29187b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58a0de-b6d1-4a33-8894-74b7b33e5a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca257b-7570-428d-a0fe-510febdb9727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e5acf-9c33-4ef7-b17c-ad8077f16d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3dd2f7-e672-4380-a5f6-e77503c8f2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529c700-af68-4abb-ba14-61c4e877fb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a11d9-59e3-40e4-a16b-bfc5ad993fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
