{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733e69cd-23f6-4870-9e8e-b7c2186b3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import whisper\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2ccb1d-58c1-4b1a-b33d-f0285558d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c52234-0171-44e5-848c-898dabd02452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whisper_model = whisper.load_model(\"base\").to(device).eval()\n",
    "whisper_model = whisper.load_model(\"medium\").to(device).eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1f8b7f-e7ee-44a4-badf-2fa4dbaa4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def process_audio_path(clean_path, base_dir=\"data/somos/audios\"):\n",
    "    return os.path.join(base_dir, clean_path.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "# Dataset Class\n",
    "class SOMOSDataset(Dataset):\n",
    "    def __init__(self, json_file, base_dir=\"data/somos/audios\"):\n",
    "        self.samples = load_json(json_file)\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        text = sample[\"text\"]\n",
    "        mos = int(float(sample[\"mos\"]))\n",
    "        label = torch.tensor(mos - 1, dtype=torch.long)\n",
    "\n",
    "        audio_path = process_audio_path(sample[\"clean path\"], self.base_dir)\n",
    "\n",
    "        return audio_path, text, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    audio_paths, texts, labels = zip(*batch)\n",
    "\n",
    "    audios = [whisper.load_audio(path) for path in audio_paths]\n",
    "    audios = [whisper.pad_or_trim(audio) for audio in audios]\n",
    "    mel_spectrograms = [whisper.log_mel_spectrogram(audio).to(device) for audio in audios]\n",
    "    mel_spectrograms = torch.stack(mel_spectrograms)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        audio_embeddings = whisper_model.encoder(mel_spectrograms).mean(dim=1)  # Batch audio embeddings\n",
    "\n",
    "    inputs = tokenizer(list(texts), return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = bert_model(**inputs).last_hidden_state[:, 0, :]  # Batch text embeddings\n",
    "\n",
    "    labels = torch.stack(labels).to(device)\n",
    "\n",
    "    return audio_embeddings, text_embeddings, labels\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, hidden_dim, num_classes, dropout_rate=0.1):\n",
    "        super(FusionClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(audio_dim + text_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        fused = torch.cat([audio_emb, text_emb], dim=1)\n",
    "        return self.fc(fused)\n",
    "\n",
    "\n",
    "def save_model(model, epoch, best_acc, save_path=\"models\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model_path = os.path.join(save_path, f\"model_epoch_{epoch}.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    best_model_path = os.path.join(save_path, \"best_model.pth\")\n",
    "    if best_acc:\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8881b084-fe84-47f4-8977-c903d161e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def compute_class_weights(dataset, num_classes=5):\n",
    "    labels = [int(float(sample[\"mos\"])) - 1 for sample in dataset.samples]  # Convert to 0-based index\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    total_samples = len(labels)\n",
    "    class_weights = {cls: total_samples / (num_classes * count) for cls, count in class_counts.items()}\n",
    "    \n",
    "    # Convert to a tensor\n",
    "    weights = torch.tensor([class_weights[i] for i in range(num_classes)], dtype=torch.float).to(device)\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3fb4e3-9fb6-4c7b-ae3a-f19f4ca5a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def create_weighted_sampler(dataset, num_classes=5):\n",
    "    # Compute frequency of each label\n",
    "    labels = [int(float(sample[\"mos\"])) - 1 for sample in dataset.samples]\n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    # Assign weights: lower for frequent classes, higher for rare ones\n",
    "    sample_weights = [total / counts[label] for label in labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    return sampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e26c06-ea30-442a-83a1-e70251fc70eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model architecture:\n",
      " Whisper(\n",
      "  (encoder): AudioEncoder(\n",
      "    (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TextDecoder(\n",
      "    (token_embedding): Embedding(51865, 1024)\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x ResidualAttentionBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (cross_attn): MultiHeadAttention(\n",
      "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "        (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "encoder AudioEncoder(\n",
      "  (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (blocks): ModuleList(\n",
      "    (0-23): 24 x ResidualAttentionBlock(\n",
      "      (attn): MultiHeadAttention(\n",
      "        (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      )\n",
      "      (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.conv1 Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "encoder.conv2 Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "encoder.blocks ModuleList(\n",
      "  (0-23): 24 x ResidualAttentionBlock(\n",
      "    (attn): MultiHeadAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "encoder.blocks.0 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.0.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.0.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.0.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.0.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.0.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.0.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.0.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.0.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.0.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.0.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.0.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.1 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.1.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.1.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.1.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.1.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.1.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.1.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.1.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.1.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.1.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.1.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.1.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.2 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.2.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.2.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.2.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.2.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.2.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.2.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.2.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.2.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.2.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.2.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.2.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.3 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.3.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.3.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.3.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.3.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.3.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.3.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.3.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.3.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.3.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.3.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.3.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.4 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.4.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.4.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.4.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.4.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.4.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.4.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.4.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.4.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.4.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.4.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.4.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.5 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.5.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.5.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.5.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.5.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.5.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.5.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.5.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.5.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.5.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.5.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.5.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.6 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.6.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.6.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.6.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.6.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.6.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.6.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.6.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.6.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.6.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.6.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.6.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.7 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.7.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.7.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.7.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.7.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.7.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.7.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.7.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.7.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.7.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.7.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.7.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.8 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.8.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.8.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.8.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.8.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.8.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.8.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.8.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.8.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.8.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.8.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.8.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.9 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.9.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.9.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.9.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.9.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.9.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.9.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.9.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.9.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.9.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.9.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.9.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.10 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.10.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.10.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.10.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.10.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.10.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.10.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.10.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.10.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.10.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.10.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.10.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.11 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.11.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.11.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.11.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.11.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.11.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.11.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.11.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.11.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.11.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.11.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.11.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.12 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.12.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.12.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.12.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.12.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.12.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.12.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.12.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.12.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.12.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.12.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.12.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.13 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.13.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.13.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.13.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.13.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.13.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.13.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.13.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.13.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.13.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.13.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.13.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.14 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.14.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.14.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.14.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.14.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.14.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.14.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.14.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.14.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.14.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.14.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.14.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.15 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.15.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.15.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.15.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.15.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.15.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.15.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.15.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.15.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.15.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.15.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.15.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.16 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.16.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.16.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.16.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.16.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.16.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.16.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.16.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.16.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.16.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.16.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.16.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.17 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.17.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.17.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.17.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.17.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.17.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.17.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.17.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.17.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.17.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.17.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.17.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.18 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.18.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.18.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.18.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.18.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.18.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.18.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.18.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.18.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.18.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.18.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.18.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.19 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.19.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.19.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.19.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.19.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.19.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.19.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.19.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.19.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.19.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.19.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.19.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.20 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.20.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.20.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.20.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.20.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.20.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.20.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.20.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.20.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.20.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.20.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.20.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.21 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.21.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.21.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.21.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.21.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.21.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.21.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.21.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.21.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.21.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.21.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.21.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.22 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.22.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.22.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.22.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.22.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.22.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.22.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.22.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.22.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.22.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.22.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.22.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.23 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "encoder.blocks.23.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.23.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.23.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "encoder.blocks.23.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.23.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "encoder.blocks.23.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.blocks.23.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "encoder.blocks.23.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "encoder.blocks.23.mlp.1 GELU(approximate='none')\n",
      "encoder.blocks.23.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "encoder.blocks.23.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "encoder.ln_post LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder TextDecoder(\n",
      "  (token_embedding): Embedding(51865, 1024)\n",
      "  (blocks): ModuleList(\n",
      "    (0-23): 24 x ResidualAttentionBlock(\n",
      "      (attn): MultiHeadAttention(\n",
      "        (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (cross_attn): MultiHeadAttention(\n",
      "        (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      )\n",
      "      (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.token_embedding Embedding(51865, 1024)\n",
      "decoder.blocks ModuleList(\n",
      "  (0-23): 24 x ResidualAttentionBlock(\n",
      "    (attn): MultiHeadAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (cross_attn): MultiHeadAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    )\n",
      "    (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "decoder.blocks.0 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.0.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.0.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.0.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.0.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.0.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.0.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.0.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.0.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.0.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.0.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.0.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.0.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.1 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.1.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.1.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.1.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.1.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.1.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.1.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.1.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.1.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.1.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.1.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.1.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.1.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.2 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.2.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.2.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.2.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.2.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.2.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.2.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.2.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.2.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.2.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.2.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.2.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.2.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.3 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.3.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.3.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.3.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.3.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.3.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.3.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.3.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.3.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.3.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.3.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.3.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.3.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.4 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.4.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.4.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.4.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.4.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.4.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.4.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.4.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.4.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.4.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.4.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.4.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.4.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.5 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.5.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.5.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.5.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.5.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.5.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.5.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.5.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.5.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.5.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.5.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.5.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.5.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.6 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.6.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.6.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.6.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.6.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.6.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.6.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.6.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.6.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.6.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.6.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.6.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.6.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.7 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.7.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.7.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.7.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.7.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.7.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.7.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.7.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.7.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.7.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.7.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.7.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.7.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.8 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.8.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.8.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.8.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.8.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.8.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.8.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.8.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.8.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.8.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.8.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.8.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.8.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.9 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.9.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.9.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.9.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.9.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.9.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.9.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.9.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.9.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.9.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.9.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.9.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.9.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.10 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.10.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.10.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.10.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.10.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.10.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.10.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.10.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.10.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.10.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.10.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.10.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.10.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.11 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.11.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.11.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.11.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.11.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.11.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.11.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.11.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.11.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.11.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.11.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.11.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.11.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.12 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.12.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.12.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.12.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.12.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.12.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.12.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.12.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.12.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.12.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.12.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.12.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.12.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.13 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.13.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.13.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.13.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.13.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.13.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.13.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.13.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.13.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.13.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.13.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.13.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.13.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.14 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.14.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.14.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.14.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.14.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.14.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.14.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.14.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.14.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.14.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.14.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.14.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.14.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.15 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.15.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.15.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.15.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.15.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.15.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.15.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.15.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.15.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.15.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.15.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.15.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.15.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.16 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.16.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.16.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.16.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.16.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.16.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.16.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.16.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.16.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.16.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.16.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.16.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.16.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.17 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.17.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.17.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.17.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.17.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.17.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.17.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.17.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.17.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.17.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.17.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.17.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.17.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.18 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.18.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.18.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.18.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.18.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.18.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.18.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.18.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.18.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.18.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.18.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.18.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.18.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.19 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.19.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.19.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.19.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.19.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.19.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.19.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.19.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.19.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.19.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.19.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.19.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.19.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.20 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.20.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.20.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.20.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.20.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.20.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.20.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.20.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.20.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.20.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.20.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.20.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.20.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.21 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.21.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.21.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.21.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.21.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.21.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.21.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.21.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.21.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.21.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.21.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.21.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.21.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.22 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.22.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.22.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.22.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.22.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.22.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.22.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.22.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.22.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.22.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.22.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.22.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.22.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.23 ResidualAttentionBlock(\n",
      "  (attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cross_attn): MultiHeadAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  )\n",
      "  (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "decoder.blocks.23.attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.23.attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.23.attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.23.cross_attn MultiHeadAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.23.cross_attn.query Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.cross_attn.key Linear(in_features=1024, out_features=1024, bias=False)\n",
      "decoder.blocks.23.cross_attn.value Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.cross_attn.out Linear(in_features=1024, out_features=1024, bias=True)\n",
      "decoder.blocks.23.cross_attn_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.blocks.23.mlp Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      ")\n",
      "decoder.blocks.23.mlp.0 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "decoder.blocks.23.mlp.1 GELU(approximate='none')\n",
      "decoder.blocks.23.mlp.2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "decoder.blocks.23.mlp_ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "decoder.ln LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Full model architecture:\")\n",
    "for name, module in whisper_model.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "055831e0-080c-483b-b84b-0c17ce43a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in whisper_model.encoder.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b21210-7d87-417c-a810-4ad3df715ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total encoder blocks: 24\n"
     ]
    }
   ],
   "source": [
    "encoder_blocks = whisper_model.encoder.blocks\n",
    "total_blocks = len(encoder_blocks)\n",
    "print(f\"Total encoder blocks: {total_blocks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee4a6a9-fd2e-4680-b9f6-883aa65d7849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing encoder blocks with index >= 12\n"
     ]
    }
   ],
   "source": [
    "threshold = total_blocks // 2  # unfreeze blocks with index >= threshold\n",
    "print(f\"Unfreezing encoder blocks with index >= {threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d63eb76b-ff76-45cf-bea7-f534ab89ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, block in enumerate(encoder_blocks):\n",
    "    if idx >= threshold:\n",
    "        for name, param in block.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c014d7-253d-42b0-a34b-e81bdf63a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(whisper_model.encoder, \"final_layer_norm\"):\n",
    "    for param in whisper_model.encoder.final_layer_norm.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4780bac1-3959-42ad-9e2d-5617d6ceb803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unfrozen parameters in the Whisper encoder:\n",
      "blocks.12.attn.query.weight\n",
      "blocks.12.attn.query.bias\n",
      "blocks.12.attn.key.weight\n",
      "blocks.12.attn.value.weight\n",
      "blocks.12.attn.value.bias\n",
      "blocks.12.attn.out.weight\n",
      "blocks.12.attn.out.bias\n",
      "blocks.12.attn_ln.weight\n",
      "blocks.12.attn_ln.bias\n",
      "blocks.12.mlp.0.weight\n",
      "blocks.12.mlp.0.bias\n",
      "blocks.12.mlp.2.weight\n",
      "blocks.12.mlp.2.bias\n",
      "blocks.12.mlp_ln.weight\n",
      "blocks.12.mlp_ln.bias\n",
      "blocks.13.attn.query.weight\n",
      "blocks.13.attn.query.bias\n",
      "blocks.13.attn.key.weight\n",
      "blocks.13.attn.value.weight\n",
      "blocks.13.attn.value.bias\n",
      "blocks.13.attn.out.weight\n",
      "blocks.13.attn.out.bias\n",
      "blocks.13.attn_ln.weight\n",
      "blocks.13.attn_ln.bias\n",
      "blocks.13.mlp.0.weight\n",
      "blocks.13.mlp.0.bias\n",
      "blocks.13.mlp.2.weight\n",
      "blocks.13.mlp.2.bias\n",
      "blocks.13.mlp_ln.weight\n",
      "blocks.13.mlp_ln.bias\n",
      "blocks.14.attn.query.weight\n",
      "blocks.14.attn.query.bias\n",
      "blocks.14.attn.key.weight\n",
      "blocks.14.attn.value.weight\n",
      "blocks.14.attn.value.bias\n",
      "blocks.14.attn.out.weight\n",
      "blocks.14.attn.out.bias\n",
      "blocks.14.attn_ln.weight\n",
      "blocks.14.attn_ln.bias\n",
      "blocks.14.mlp.0.weight\n",
      "blocks.14.mlp.0.bias\n",
      "blocks.14.mlp.2.weight\n",
      "blocks.14.mlp.2.bias\n",
      "blocks.14.mlp_ln.weight\n",
      "blocks.14.mlp_ln.bias\n",
      "blocks.15.attn.query.weight\n",
      "blocks.15.attn.query.bias\n",
      "blocks.15.attn.key.weight\n",
      "blocks.15.attn.value.weight\n",
      "blocks.15.attn.value.bias\n",
      "blocks.15.attn.out.weight\n",
      "blocks.15.attn.out.bias\n",
      "blocks.15.attn_ln.weight\n",
      "blocks.15.attn_ln.bias\n",
      "blocks.15.mlp.0.weight\n",
      "blocks.15.mlp.0.bias\n",
      "blocks.15.mlp.2.weight\n",
      "blocks.15.mlp.2.bias\n",
      "blocks.15.mlp_ln.weight\n",
      "blocks.15.mlp_ln.bias\n",
      "blocks.16.attn.query.weight\n",
      "blocks.16.attn.query.bias\n",
      "blocks.16.attn.key.weight\n",
      "blocks.16.attn.value.weight\n",
      "blocks.16.attn.value.bias\n",
      "blocks.16.attn.out.weight\n",
      "blocks.16.attn.out.bias\n",
      "blocks.16.attn_ln.weight\n",
      "blocks.16.attn_ln.bias\n",
      "blocks.16.mlp.0.weight\n",
      "blocks.16.mlp.0.bias\n",
      "blocks.16.mlp.2.weight\n",
      "blocks.16.mlp.2.bias\n",
      "blocks.16.mlp_ln.weight\n",
      "blocks.16.mlp_ln.bias\n",
      "blocks.17.attn.query.weight\n",
      "blocks.17.attn.query.bias\n",
      "blocks.17.attn.key.weight\n",
      "blocks.17.attn.value.weight\n",
      "blocks.17.attn.value.bias\n",
      "blocks.17.attn.out.weight\n",
      "blocks.17.attn.out.bias\n",
      "blocks.17.attn_ln.weight\n",
      "blocks.17.attn_ln.bias\n",
      "blocks.17.mlp.0.weight\n",
      "blocks.17.mlp.0.bias\n",
      "blocks.17.mlp.2.weight\n",
      "blocks.17.mlp.2.bias\n",
      "blocks.17.mlp_ln.weight\n",
      "blocks.17.mlp_ln.bias\n",
      "blocks.18.attn.query.weight\n",
      "blocks.18.attn.query.bias\n",
      "blocks.18.attn.key.weight\n",
      "blocks.18.attn.value.weight\n",
      "blocks.18.attn.value.bias\n",
      "blocks.18.attn.out.weight\n",
      "blocks.18.attn.out.bias\n",
      "blocks.18.attn_ln.weight\n",
      "blocks.18.attn_ln.bias\n",
      "blocks.18.mlp.0.weight\n",
      "blocks.18.mlp.0.bias\n",
      "blocks.18.mlp.2.weight\n",
      "blocks.18.mlp.2.bias\n",
      "blocks.18.mlp_ln.weight\n",
      "blocks.18.mlp_ln.bias\n",
      "blocks.19.attn.query.weight\n",
      "blocks.19.attn.query.bias\n",
      "blocks.19.attn.key.weight\n",
      "blocks.19.attn.value.weight\n",
      "blocks.19.attn.value.bias\n",
      "blocks.19.attn.out.weight\n",
      "blocks.19.attn.out.bias\n",
      "blocks.19.attn_ln.weight\n",
      "blocks.19.attn_ln.bias\n",
      "blocks.19.mlp.0.weight\n",
      "blocks.19.mlp.0.bias\n",
      "blocks.19.mlp.2.weight\n",
      "blocks.19.mlp.2.bias\n",
      "blocks.19.mlp_ln.weight\n",
      "blocks.19.mlp_ln.bias\n",
      "blocks.20.attn.query.weight\n",
      "blocks.20.attn.query.bias\n",
      "blocks.20.attn.key.weight\n",
      "blocks.20.attn.value.weight\n",
      "blocks.20.attn.value.bias\n",
      "blocks.20.attn.out.weight\n",
      "blocks.20.attn.out.bias\n",
      "blocks.20.attn_ln.weight\n",
      "blocks.20.attn_ln.bias\n",
      "blocks.20.mlp.0.weight\n",
      "blocks.20.mlp.0.bias\n",
      "blocks.20.mlp.2.weight\n",
      "blocks.20.mlp.2.bias\n",
      "blocks.20.mlp_ln.weight\n",
      "blocks.20.mlp_ln.bias\n",
      "blocks.21.attn.query.weight\n",
      "blocks.21.attn.query.bias\n",
      "blocks.21.attn.key.weight\n",
      "blocks.21.attn.value.weight\n",
      "blocks.21.attn.value.bias\n",
      "blocks.21.attn.out.weight\n",
      "blocks.21.attn.out.bias\n",
      "blocks.21.attn_ln.weight\n",
      "blocks.21.attn_ln.bias\n",
      "blocks.21.mlp.0.weight\n",
      "blocks.21.mlp.0.bias\n",
      "blocks.21.mlp.2.weight\n",
      "blocks.21.mlp.2.bias\n",
      "blocks.21.mlp_ln.weight\n",
      "blocks.21.mlp_ln.bias\n",
      "blocks.22.attn.query.weight\n",
      "blocks.22.attn.query.bias\n",
      "blocks.22.attn.key.weight\n",
      "blocks.22.attn.value.weight\n",
      "blocks.22.attn.value.bias\n",
      "blocks.22.attn.out.weight\n",
      "blocks.22.attn.out.bias\n",
      "blocks.22.attn_ln.weight\n",
      "blocks.22.attn_ln.bias\n",
      "blocks.22.mlp.0.weight\n",
      "blocks.22.mlp.0.bias\n",
      "blocks.22.mlp.2.weight\n",
      "blocks.22.mlp.2.bias\n",
      "blocks.22.mlp_ln.weight\n",
      "blocks.22.mlp_ln.bias\n",
      "blocks.23.attn.query.weight\n",
      "blocks.23.attn.query.bias\n",
      "blocks.23.attn.key.weight\n",
      "blocks.23.attn.value.weight\n",
      "blocks.23.attn.value.bias\n",
      "blocks.23.attn.out.weight\n",
      "blocks.23.attn.out.bias\n",
      "blocks.23.attn_ln.weight\n",
      "blocks.23.attn_ln.bias\n",
      "blocks.23.mlp.0.weight\n",
      "blocks.23.mlp.0.bias\n",
      "blocks.23.mlp.2.weight\n",
      "blocks.23.mlp.2.bias\n",
      "blocks.23.mlp_ln.weight\n",
      "blocks.23.mlp_ln.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnfrozen parameters in the Whisper encoder:\")\n",
    "for name, param in whisper_model.encoder.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee83f81-b5b0-4c3c-8d27-54c07bf2a49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99ef3d95-e539-443a-a985-677c52a328e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_json = \"data/somos/audios/train.json\"\n",
    "    test_json = \"data/somos/audios/test.json\"\n",
    "\n",
    "    train_dataset = SOMOSDataset(train_json)\n",
    "    test_dataset = SOMOSDataset(test_json)\n",
    "\n",
    "    # class_weights = compute_class_weights(train_dataset)\n",
    "    # print(class_weights)\n",
    "\n",
    "    train_sampler = create_weighted_sampler(train_dataset)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, sampler=train_sampler, collate_fn=collate_fn)\n",
    "    \n",
    "    # train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    dummy_audio, dummy_text, _ = next(iter(train_loader))\n",
    "    audio_dim, text_dim = dummy_audio.shape[1], dummy_text.shape[1]\n",
    "    num_classes = 5\n",
    "\n",
    "    model = FusionClassifier(audio_dim, text_dim, hidden_dim=256, num_classes=num_classes).to(device)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    num_epochs = 10\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct_preds, total_samples = 0.0, 0, 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "        for audio_emb, text_emb, labels in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(audio_emb, text_emb)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * audio_emb.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_acc = 100 * correct_preds / total_samples\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {running_loss/total_samples:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "         # Evaluation\n",
    "        model.eval()\n",
    "        test_loss, correct_preds, total_samples = 0.0, 0, 0\n",
    "        test_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
    "            for audio_emb, text_emb, labels in test_pbar:\n",
    "                audio_emb = audio_emb.to(device)\n",
    "                text_emb = text_emb.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(audio_emb, text_emb)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item() * audio_emb.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct_preds += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "                test_predictions.extend(zip(labels.cpu().tolist(), preds.cpu().tolist()))\n",
    "\n",
    "                test_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        test_acc = 100 * correct_preds / total_samples\n",
    "        avg_test_loss = test_loss / total_samples\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Val Loss: {avg_test_loss:.4f} | Val Acc: {test_acc:.2f}%\")\n",
    "\n",
    "        print(\"\\nSample Predictions (Real MOS vs Predicted MOS):\")\n",
    "        for i, (real_mos, pred_mos) in enumerate(test_predictions[:20]):\n",
    "            print(f\"Example {i+1}: Real MOS = {real_mos + 1}, Predicted MOS = {pred_mos + 1}\")  # Convert back to 1-5 scale\n",
    "\n",
    "        save_model(model, epoch + 1, test_acc > best_acc)\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "\n",
    "    print(\"Training complete! Best validation accuracy:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b2b68de-f397-49a0-9c26-fa5118ea0d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\q\\AppData\\Local\\Temp\\ipykernel_33264\\46783232.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 1 Training:   0%|                                                                       | 0/3525 [00:00<?, ?it/s]C:\\Users\\q\\AppData\\Local\\Temp\\ipykernel_33264\\46783232.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.6198 | Train Acc: 23.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Val Loss: 1.6099 | Val Acc: 20.83%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 4\n",
      "Example 2: Real MOS = 3, Predicted MOS = 2\n",
      "Example 3: Real MOS = 5, Predicted MOS = 4\n",
      "Example 4: Real MOS = 2, Predicted MOS = 3\n",
      "Example 5: Real MOS = 3, Predicted MOS = 3\n",
      "Example 6: Real MOS = 4, Predicted MOS = 4\n",
      "Example 7: Real MOS = 3, Predicted MOS = 4\n",
      "Example 8: Real MOS = 3, Predicted MOS = 4\n",
      "Example 9: Real MOS = 3, Predicted MOS = 5\n",
      "Example 10: Real MOS = 3, Predicted MOS = 5\n",
      "Example 11: Real MOS = 5, Predicted MOS = 4\n",
      "Example 12: Real MOS = 5, Predicted MOS = 2\n",
      "Example 13: Real MOS = 4, Predicted MOS = 3\n",
      "Example 14: Real MOS = 3, Predicted MOS = 1\n",
      "Example 15: Real MOS = 4, Predicted MOS = 2\n",
      "Example 16: Real MOS = 5, Predicted MOS = 1\n",
      "Example 17: Real MOS = 5, Predicted MOS = 4\n",
      "Example 18: Real MOS = 4, Predicted MOS = 4\n",
      "Example 19: Real MOS = 5, Predicted MOS = 5\n",
      "Example 20: Real MOS = 2, Predicted MOS = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 1.5887 | Train Acc: 25.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Val Loss: 1.6492 | Val Acc: 17.00%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 2\n",
      "Example 2: Real MOS = 3, Predicted MOS = 2\n",
      "Example 3: Real MOS = 5, Predicted MOS = 4\n",
      "Example 4: Real MOS = 2, Predicted MOS = 3\n",
      "Example 5: Real MOS = 3, Predicted MOS = 3\n",
      "Example 6: Real MOS = 4, Predicted MOS = 4\n",
      "Example 7: Real MOS = 3, Predicted MOS = 4\n",
      "Example 8: Real MOS = 3, Predicted MOS = 4\n",
      "Example 9: Real MOS = 3, Predicted MOS = 5\n",
      "Example 10: Real MOS = 3, Predicted MOS = 5\n",
      "Example 11: Real MOS = 5, Predicted MOS = 3\n",
      "Example 12: Real MOS = 5, Predicted MOS = 1\n",
      "Example 13: Real MOS = 4, Predicted MOS = 1\n",
      "Example 14: Real MOS = 3, Predicted MOS = 2\n",
      "Example 15: Real MOS = 4, Predicted MOS = 1\n",
      "Example 16: Real MOS = 5, Predicted MOS = 2\n",
      "Example 17: Real MOS = 5, Predicted MOS = 4\n",
      "Example 18: Real MOS = 4, Predicted MOS = 3\n",
      "Example 19: Real MOS = 5, Predicted MOS = 5\n",
      "Example 20: Real MOS = 2, Predicted MOS = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 1.5755 | Train Acc: 26.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Val Loss: 1.6018 | Val Acc: 20.90%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 5\n",
      "Example 2: Real MOS = 3, Predicted MOS = 3\n",
      "Example 3: Real MOS = 5, Predicted MOS = 4\n",
      "Example 4: Real MOS = 2, Predicted MOS = 3\n",
      "Example 5: Real MOS = 3, Predicted MOS = 3\n",
      "Example 6: Real MOS = 4, Predicted MOS = 2\n",
      "Example 7: Real MOS = 3, Predicted MOS = 5\n",
      "Example 8: Real MOS = 3, Predicted MOS = 1\n",
      "Example 9: Real MOS = 3, Predicted MOS = 3\n",
      "Example 10: Real MOS = 3, Predicted MOS = 2\n",
      "Example 11: Real MOS = 5, Predicted MOS = 3\n",
      "Example 12: Real MOS = 5, Predicted MOS = 3\n",
      "Example 13: Real MOS = 4, Predicted MOS = 3\n",
      "Example 14: Real MOS = 3, Predicted MOS = 3\n",
      "Example 15: Real MOS = 4, Predicted MOS = 1\n",
      "Example 16: Real MOS = 5, Predicted MOS = 2\n",
      "Example 17: Real MOS = 5, Predicted MOS = 2\n",
      "Example 18: Real MOS = 4, Predicted MOS = 5\n",
      "Example 19: Real MOS = 5, Predicted MOS = 2\n",
      "Example 20: Real MOS = 2, Predicted MOS = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 1.5603 | Train Acc: 28.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Val Loss: 1.6590 | Val Acc: 15.30%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 1\n",
      "Example 2: Real MOS = 3, Predicted MOS = 1\n",
      "Example 3: Real MOS = 5, Predicted MOS = 2\n",
      "Example 4: Real MOS = 2, Predicted MOS = 1\n",
      "Example 5: Real MOS = 3, Predicted MOS = 1\n",
      "Example 6: Real MOS = 4, Predicted MOS = 2\n",
      "Example 7: Real MOS = 3, Predicted MOS = 2\n",
      "Example 8: Real MOS = 3, Predicted MOS = 1\n",
      "Example 9: Real MOS = 3, Predicted MOS = 5\n",
      "Example 10: Real MOS = 3, Predicted MOS = 5\n",
      "Example 11: Real MOS = 5, Predicted MOS = 2\n",
      "Example 12: Real MOS = 5, Predicted MOS = 1\n",
      "Example 13: Real MOS = 4, Predicted MOS = 1\n",
      "Example 14: Real MOS = 3, Predicted MOS = 2\n",
      "Example 15: Real MOS = 4, Predicted MOS = 1\n",
      "Example 16: Real MOS = 5, Predicted MOS = 1\n",
      "Example 17: Real MOS = 5, Predicted MOS = 2\n",
      "Example 18: Real MOS = 4, Predicted MOS = 2\n",
      "Example 19: Real MOS = 5, Predicted MOS = 2\n",
      "Example 20: Real MOS = 2, Predicted MOS = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Train Loss: 1.5397 | Train Acc: 29.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Val Loss: 1.6985 | Val Acc: 14.23%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 1\n",
      "Example 2: Real MOS = 3, Predicted MOS = 1\n",
      "Example 3: Real MOS = 5, Predicted MOS = 2\n",
      "Example 4: Real MOS = 2, Predicted MOS = 1\n",
      "Example 5: Real MOS = 3, Predicted MOS = 1\n",
      "Example 6: Real MOS = 4, Predicted MOS = 4\n",
      "Example 7: Real MOS = 3, Predicted MOS = 4\n",
      "Example 8: Real MOS = 3, Predicted MOS = 4\n",
      "Example 9: Real MOS = 3, Predicted MOS = 5\n",
      "Example 10: Real MOS = 3, Predicted MOS = 5\n",
      "Example 11: Real MOS = 5, Predicted MOS = 3\n",
      "Example 12: Real MOS = 5, Predicted MOS = 1\n",
      "Example 13: Real MOS = 4, Predicted MOS = 1\n",
      "Example 14: Real MOS = 3, Predicted MOS = 2\n",
      "Example 15: Real MOS = 4, Predicted MOS = 1\n",
      "Example 16: Real MOS = 5, Predicted MOS = 1\n",
      "Example 17: Real MOS = 5, Predicted MOS = 4\n",
      "Example 18: Real MOS = 4, Predicted MOS = 2\n",
      "Example 19: Real MOS = 5, Predicted MOS = 2\n",
      "Example 20: Real MOS = 2, Predicted MOS = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Train Loss: 1.5300 | Train Acc: 30.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Val Loss: 1.6262 | Val Acc: 19.43%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 5\n",
      "Example 2: Real MOS = 3, Predicted MOS = 5\n",
      "Example 3: Real MOS = 5, Predicted MOS = 2\n",
      "Example 4: Real MOS = 2, Predicted MOS = 3\n",
      "Example 5: Real MOS = 3, Predicted MOS = 3\n",
      "Example 6: Real MOS = 4, Predicted MOS = 4\n",
      "Example 7: Real MOS = 3, Predicted MOS = 4\n",
      "Example 8: Real MOS = 3, Predicted MOS = 4\n",
      "Example 9: Real MOS = 3, Predicted MOS = 5\n",
      "Example 10: Real MOS = 3, Predicted MOS = 5\n",
      "Example 11: Real MOS = 5, Predicted MOS = 3\n",
      "Example 12: Real MOS = 5, Predicted MOS = 3\n",
      "Example 13: Real MOS = 4, Predicted MOS = 1\n",
      "Example 14: Real MOS = 3, Predicted MOS = 2\n",
      "Example 15: Real MOS = 4, Predicted MOS = 2\n",
      "Example 16: Real MOS = 5, Predicted MOS = 2\n",
      "Example 17: Real MOS = 5, Predicted MOS = 4\n",
      "Example 18: Real MOS = 4, Predicted MOS = 5\n",
      "Example 19: Real MOS = 5, Predicted MOS = 4\n",
      "Example 20: Real MOS = 2, Predicted MOS = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Train Loss: 1.5111 | Train Acc: 31.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Val Loss: 1.5944 | Val Acc: 16.90%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 1\n",
      "Example 2: Real MOS = 3, Predicted MOS = 1\n",
      "Example 3: Real MOS = 5, Predicted MOS = 2\n",
      "Example 4: Real MOS = 2, Predicted MOS = 3\n",
      "Example 5: Real MOS = 3, Predicted MOS = 3\n",
      "Example 6: Real MOS = 4, Predicted MOS = 3\n",
      "Example 7: Real MOS = 3, Predicted MOS = 2\n",
      "Example 8: Real MOS = 3, Predicted MOS = 3\n",
      "Example 9: Real MOS = 3, Predicted MOS = 5\n",
      "Example 10: Real MOS = 3, Predicted MOS = 5\n",
      "Example 11: Real MOS = 5, Predicted MOS = 2\n",
      "Example 12: Real MOS = 5, Predicted MOS = 4\n",
      "Example 13: Real MOS = 4, Predicted MOS = 3\n",
      "Example 14: Real MOS = 3, Predicted MOS = 2\n",
      "Example 15: Real MOS = 4, Predicted MOS = 2\n",
      "Example 16: Real MOS = 5, Predicted MOS = 2\n",
      "Example 17: Real MOS = 5, Predicted MOS = 3\n",
      "Example 18: Real MOS = 4, Predicted MOS = 4\n",
      "Example 19: Real MOS = 5, Predicted MOS = 2\n",
      "Example 20: Real MOS = 2, Predicted MOS = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Train Loss: 1.5014 | Train Acc: 32.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Val Loss: 1.5896 | Val Acc: 16.77%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 5\n",
      "Example 2: Real MOS = 3, Predicted MOS = 2\n",
      "Example 3: Real MOS = 5, Predicted MOS = 2\n",
      "Example 4: Real MOS = 2, Predicted MOS = 3\n",
      "Example 5: Real MOS = 3, Predicted MOS = 3\n",
      "Example 6: Real MOS = 4, Predicted MOS = 4\n",
      "Example 7: Real MOS = 3, Predicted MOS = 2\n",
      "Example 8: Real MOS = 3, Predicted MOS = 4\n",
      "Example 9: Real MOS = 3, Predicted MOS = 5\n",
      "Example 10: Real MOS = 3, Predicted MOS = 5\n",
      "Example 11: Real MOS = 5, Predicted MOS = 3\n",
      "Example 12: Real MOS = 5, Predicted MOS = 3\n",
      "Example 13: Real MOS = 4, Predicted MOS = 3\n",
      "Example 14: Real MOS = 3, Predicted MOS = 2\n",
      "Example 15: Real MOS = 4, Predicted MOS = 2\n",
      "Example 16: Real MOS = 5, Predicted MOS = 2\n",
      "Example 17: Real MOS = 5, Predicted MOS = 4\n",
      "Example 18: Real MOS = 4, Predicted MOS = 5\n",
      "Example 19: Real MOS = 5, Predicted MOS = 2\n",
      "Example 20: Real MOS = 2, Predicted MOS = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Train Loss: 1.4876 | Train Acc: 32.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Val Loss: 1.6245 | Val Acc: 17.33%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 1\n",
      "Example 2: Real MOS = 3, Predicted MOS = 1\n",
      "Example 3: Real MOS = 5, Predicted MOS = 1\n",
      "Example 4: Real MOS = 2, Predicted MOS = 1\n",
      "Example 5: Real MOS = 3, Predicted MOS = 1\n",
      "Example 6: Real MOS = 4, Predicted MOS = 4\n",
      "Example 7: Real MOS = 3, Predicted MOS = 4\n",
      "Example 8: Real MOS = 3, Predicted MOS = 4\n",
      "Example 9: Real MOS = 3, Predicted MOS = 5\n",
      "Example 10: Real MOS = 3, Predicted MOS = 5\n",
      "Example 11: Real MOS = 5, Predicted MOS = 2\n",
      "Example 12: Real MOS = 5, Predicted MOS = 4\n",
      "Example 13: Real MOS = 4, Predicted MOS = 3\n",
      "Example 14: Real MOS = 3, Predicted MOS = 3\n",
      "Example 15: Real MOS = 4, Predicted MOS = 2\n",
      "Example 16: Real MOS = 5, Predicted MOS = 3\n",
      "Example 17: Real MOS = 5, Predicted MOS = 4\n",
      "Example 18: Real MOS = 4, Predicted MOS = 3\n",
      "Example 19: Real MOS = 5, Predicted MOS = 5\n",
      "Example 20: Real MOS = 2, Predicted MOS = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Train Loss: 1.4730 | Train Acc: 33.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Val Loss: 1.6028 | Val Acc: 18.77%\n",
      "\n",
      "Sample Predictions (Real MOS vs Predicted MOS):\n",
      "Example 1: Real MOS = 4, Predicted MOS = 1\n",
      "Example 2: Real MOS = 3, Predicted MOS = 1\n",
      "Example 3: Real MOS = 5, Predicted MOS = 2\n",
      "Example 4: Real MOS = 2, Predicted MOS = 1\n",
      "Example 5: Real MOS = 3, Predicted MOS = 1\n",
      "Example 6: Real MOS = 4, Predicted MOS = 2\n",
      "Example 7: Real MOS = 3, Predicted MOS = 4\n",
      "Example 8: Real MOS = 3, Predicted MOS = 2\n",
      "Example 9: Real MOS = 3, Predicted MOS = 5\n",
      "Example 10: Real MOS = 3, Predicted MOS = 5\n",
      "Example 11: Real MOS = 5, Predicted MOS = 3\n",
      "Example 12: Real MOS = 5, Predicted MOS = 3\n",
      "Example 13: Real MOS = 4, Predicted MOS = 3\n",
      "Example 14: Real MOS = 3, Predicted MOS = 2\n",
      "Example 15: Real MOS = 4, Predicted MOS = 2\n",
      "Example 16: Real MOS = 5, Predicted MOS = 5\n",
      "Example 17: Real MOS = 5, Predicted MOS = 4\n",
      "Example 18: Real MOS = 4, Predicted MOS = 5\n",
      "Example 19: Real MOS = 5, Predicted MOS = 2\n",
      "Example 20: Real MOS = 2, Predicted MOS = 2\n",
      "Training complete! Best validation accuracy: 20.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3b105-a6ad-4d79-98c9-2c531a29187b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58a0de-b6d1-4a33-8894-74b7b33e5a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca257b-7570-428d-a0fe-510febdb9727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e5acf-9c33-4ef7-b17c-ad8077f16d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3dd2f7-e672-4380-a5f6-e77503c8f2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529c700-af68-4abb-ba14-61c4e877fb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a11d9-59e3-40e4-a16b-bfc5ad993fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
